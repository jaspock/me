{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Transformer-based language models\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/lmgpt.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Notebook and code written by Juan Antonio Pérez in 2023–2024.\n",
        "\n",
        "This notebook uses the decoder-like transformer of our previous notebook to train and test a ridiculously simple language model. The size of the dataset will prevent the model from learning anything useful, but it will be enough to illustrate the basic principles of sequence generation with transformers. \n",
        "\n",
        "It is assumed that you are already familiar with the basics of PyTorch. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation\n",
        "\n",
        "The `make_batch` is a generator that yields a batch of input and output sequences. The input sequences are just a slice of the corpus, and the output sequences are the same as the input sequences, but shifted one token to the right. The generator is infinite, so it can be used in a `for` loop to iterate over the batches. The `yield` keyword is different from `return` in that it does not terminate the function, but it returns a value and pauses the execution of the function until the next time the function is called. This allows us to keep a state between calls to the function as well as to call the function unlimited times. Note that the instruction after the `yield` keyword (a `pass` instruction in this case) will be the first to be executed when the function is called again. \n",
        "\n",
        "The second parameter in `word_index.get` is the default value to return if the key is not found in the dictionary. In this case, we return the index of the special token `[UNK]`, which is the index of the unknown token in the vocabulary. This is a simple way to handle out-of-vocabulary words. Ideally, the training corpus should contain also out-of-vocabulary words so that a useful representation can be learned for the token `[UNK]`. In this notebook, however, the vocabulary includes all the words in the corpus.\n",
        "\n",
        "The inner loop of the generator creates a batch of `batch_size` sequences. The start position in the corpus is chosen randomly, and the end index is `max_len` tokens after the start index. If the end index is greater than the length of the corpus, the remaining tokens are taken from the beginning of the corpus. \n",
        "\n",
        "To avoid having to pad the sequences, we assume that the training corpus is long enough to completely fill each *line* of the batch. If the corpus is shorter, an exception is raised."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokenized_corpus]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    assert n_tokens >= max_len, f'Short corpus ({n_tokens} tokens), must be at least {max_len} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is input shifted one token to the right:\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n",
        "        pass  # this line will be executed next time the function is called"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code\n",
        "\n",
        "We load the `DecoderTransformer` class implemented in the previous notebook. If we are running this on the cloud, we download the parent notebook file from GitHub. If we are running it locally, we assume that the file is in the same directory as this notebook. The seed is also set to a fixed value to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))  # running in Google Colab?\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing\n",
        "\n",
        "Our model will be trained with a corpus contained in a single file. In our case, we will download the Tiny Shakespeare dataset made of works by William Shakespeare. \n",
        "\n",
        "The preprocessing of the corpus follows the same steps as in the previous notebook. The only difference is the addition of a few special tokens to the vocabulary. The special tokens are `[PAD]` for padding, and `[UNK]` for unknown words. `PAD` is used to fill the sequences shorter than `max_len`, but it is not used here. `[UNK]` is used to represent out-of-vocabulary words. In this notebook, however, the vocabulary includes all the words in the corpus; therefore, good representations for `[UNK]` will not be learned. Anyway, it is used at inference time to handle out-of-vocabulary words. Note also that when subword tokenization is used, unknown tokens are usually not so frequent.\n",
        "\n",
        "Different tasks may require different special tokens. For example, a multilingual model may need a special token to indicate the language of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download Tiny Shakespeare dataset:\n",
        "import urllib.request\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "chars = 10000  # number of characters to keep\n",
        "corpus = urllib.request.urlopen(url).read().decode(\"utf-8\")[:chars]\n",
        "print(corpus[:100])\n",
        "\n",
        "word_list = list(set(corpus.split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1}\n",
        "special_tokens = len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "vocab_size = len(word_index)\n",
        "print(f\"vocab_size = {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training\n",
        "\n",
        "Hopefully, having studied the other notebooks, once you reach this point, you will realize that everything sounds familiar and understandable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 4\n",
        "max_len = 32\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.001\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,  \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
        "\n",
        "model.train()\n",
        "tokenized_corpus = corpus.split()\n",
        "step = 0\n",
        "\n",
        "for inputs, outputs in make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if step % eval_steps == 0:\n",
        "        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break\n",
        "\n",
        "print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}, perplexity: {math.exp(loss.item()):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\n",
        "\n",
        "The `generate` function is used to auto-regressively continue a given prompt up until `max_len` tokens. It starts off by tokenizing the prompt and converting it to a one-sample mini-batch. Then, it iteratively predicts the next token by selecting the index with the highest probability in the output vector corresponding to the last token in the sequence. The resulting index is appended to the input sequence, and the process is repeated until the desired length is reached. Finally, the predicted tokens are converted back to words and returned as a single string.\n",
        "\n",
        "Due to the intentionally small size of the training corpus, the model will probably verbatim copy excerpts from the training corpus. \n",
        "\n",
        "📘 *Documentation:* [`torch.Tensor.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html), [`torch.Tensor.item`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html), [`torch.argmax`](https://pytorch.org/docs/stable/generated/torch.argmax.html), [`torch.Tensor.view`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_text(model, prompt, word_index, index_word, max_len, device):\n",
        "    words = prompt.split()\n",
        "    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n",
        "    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(input_ids)):\n",
        "            output = model(input)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n",
        "            predicted_word = index_word[predicted_id]\n",
        "            words.append(predicted_word)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "model.eval()\n",
        "prompt = \"O God, that robot is out of control! I tell you, friends, \"\n",
        "generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
        "\n",
        "✎ Use SentencePiece to tokenize the data.\n",
        "\n",
        "✎ Use the [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) function to implement sampling instead of greedy decoding.\n",
        "\n",
        "✎ Implement your own versions of top-$k$ and top-$p$ (nucleus) sampling.\n",
        "\n",
        "✎ Use a mini-batch of prompts at inference time to generate multiple texts in parallel.\n",
        "\n",
        "✎ Compare the original pre-norm implementation of the transformer with the post-norm implementation under this task.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
