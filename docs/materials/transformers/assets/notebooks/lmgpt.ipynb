{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Transformer-based language models\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/lmgpt.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Notebook and code written by Juan Antonio Pérez in 2023–2024.\n",
        "\n",
        "This notebook extends the decoder-like transformer of our previous notebook to train and test a ridiculously simple language model. The size of the dataset will prevent the model from learning anything useful, but it will be enough to illustrate the basic principles of sequence generation with transformers. \n",
        "\n",
        "It is assumed that you are already familiar with the basics of PyTorch. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Comentar sobre desconocidas y UNK en train y test\n",
        "- Hacer versión corta con MultiHeadAttention y LN de PyTorch\n",
        "- **Exercise**: use spm to tokenize the data \n",
        "- poner set_seed en los otros notebooks y evaluarlo\n",
        "- usar no_grad en todas la evaluaciones\n",
        "- avisar que no usaremos prefix tuning o como se llame (supongo que hay que usarla tb durante el training)\n",
        "- ejercicio: top-p sampling\n",
        "- sustituir !pip install por %pip en todos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation\n",
        "\n",
        "import torch\n",
        "\n",
        "def make_batch(corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    tokens = corpus.split()\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokens]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    batch_token_length = batch_size * max_len  # total number of tokens in a batch\n",
        "    assert n_tokens >= batch_token_length, f'Short corpus ({n_tokens} tokens), must be at least {batch_length} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is the same as input, except shifted one token to the right\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n",
        "\n",
        "The `make_batch` is a generator that yields a batch of input and output sequences. The input sequences are just a slice of the corpus, and the output sequences are the same as the input sequences, but shifted one token to the right. The generator is infinite, so it can be used in a `for` loop to iterate over the batches. The `yield` keyword is different from `return` in that it does not terminate the function, but it returns a value and pauses the execution of the function until the next time the function is called. This allows us to keep a state between calls to the function as well as to call the function unlimited times.\n",
        "\n",
        "The second parameter in `word_index.get` is the default value to return if the key is not found in the dictionary. In this case, we return the index of the special token `[UNK]`, which is the index of the unknown token in the vocabulary. This is a simple way to handle out-of-vocabulary words. \n",
        "\n",
        "The inner loop of the generator creates a batch of `batch_size` sequences. The start position in the corpus is chosen randomly, and the end index is `max_len` tokens after the start index. If the end index is greater than the length of the corpus, the remaining tokens are taken from the beginning of the corpus. \n",
        "\n",
        "We assume that the training corpus is long enough to contain at least one batch of data. If the corpus is too short, an exception is raised. [Why?]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokenized_corpus]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    batch_token_length = batch_size * max_len  # total number of tokens in a batch\n",
        "    assert n_tokens >= batch_token_length, f'Short corpus ({n_tokens} tokens), must be at least {batch_length} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is the same as input, except shifted one token to the right\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code\n",
        "\n",
        "We load the `DecoderTransformer` class implemented in the previous notebook. If we are running this on the cloud, we download the file from GitHub. If we are running it locally, we assume that the file is in the same directory as this notebook. The seed is also set to a fixed value to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))  # running in Google Colab?\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "vocab size = 863\n"
          ]
        }
      ],
      "source": [
        "# download Tiny Shakespeare dataset:\n",
        "import urllib.request\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "chars = 10000  # number of characters to keep\n",
        "corpus = urllib.request.urlopen(url).read().decode(\"utf-8\")[:chars]\n",
        "print(corpus[:100])\n",
        "\n",
        "word_list = list(set(corpus.split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1, '[EOS]': 2}\n",
        "special_tokens = len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "vocab_size = len(word_index)\n",
        "print(f\"vocab_size = {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.16M\n",
            "Step [0/1000], loss: 6.7666\n",
            "Step [100/1000], loss: 5.2885\n",
            "Step [200/1000], loss: 3.6503\n",
            "Step [300/1000], loss: 2.5783\n",
            "Step [400/1000], loss: 2.0314\n",
            "Step [500/1000], loss: 1.3531\n",
            "Step [600/1000], loss: 1.2990\n",
            "Step [700/1000], loss: 0.8839\n",
            "Step [800/1000], loss: 0.9395\n",
            "Step [900/1000], loss: 0.5503\n",
            "Step [1000/1000], loss: 0.5798\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 4\n",
        "max_len = 32\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.001\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,  \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
        "\n",
        "model.train()\n",
        "tokenized_corpus = corpus.split()\n",
        "step = 0\n",
        "\n",
        "for inputs, outputs in make_batch(tokenized_corpus, word_index, max_len, batch_size, device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if step % eval_steps == 0:\n",
        "        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break\n",
        "\n",
        "print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O God, that robot is out of control! I tell you, friends, mine honest neighbours, Will you undo yourselves? First Citizen: We cannot, sir, we are undone already. MENENIUS: I tell you,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_text(model, prompt, word_index, index_word, max_len, device):\n",
        "    words = prompt.split()\n",
        "    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n",
        "    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(input_ids)):\n",
        "            output = model(input)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n",
        "            predicted_word = index_word[predicted_id]\n",
        "            words.append(predicted_word)\n",
        "            if predicted_word == '[EOS]':\n",
        "                break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "model.eval()\n",
        "prompt = \"O God, that robot is out of control! I tell you, friends, \"\n",
        "generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
        "\n",
        "✎ Use SentencePiece to tokenize the data.\n",
        "\n",
        "✎ Use the [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) function to implement sampling instead of greedy decoding.\n",
        "\n",
        "✎ Implement your own versions of top-$k$ and top-$p$ (nucleus) sampling.\n",
        "\n",
        "✎ Use a mini-batch of prompts at inference time to generate multiple texts in parallel.\n",
        "\n",
        "✎ Compare the original pre-norm implementation of the transformer with the post-norm implementation under this task.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
