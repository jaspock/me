{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thlU7qzxespH"
   },
   "source": [
    "# Self-supervised learning of embeddings via a language model based on feed-forward neural networks\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/embeddings-ffnn.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
    "\n",
    "Notebook and code jointly written by Juan Antonio Pérez and an AI-based code generator in 2025.\n",
    "\n",
    "This notebook presents a very simple language model based on feed-forward neural networks. Its main purpose is to show not how to implement a language model, but how a self-supervised task can be solved so that word embeddings are learned as a by-product. \n",
    "\n",
    "It is assumed that you are already familiar with the basics of PyTorch, but at a absolute beginner level only. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "set_seed(42)  # to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 24\n",
    "CONTEXT_SIZE = 3\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3000\n",
    "\n",
    "# with CONTEXT=2, the model cannot learn to say which animal is being hunted or chased by another\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "raw_sentences = [\n",
    "    \"I see the dog chasing the cat .\",\n",
    "    \"I see the dog chasing the koala .\",\n",
    "    \"I see the dog hunting the cat .\",\n",
    "    \"I see the dog hunting the elephant .\",\n",
    "    \"I see the dog hunting the koala .\",\n",
    "    \"I see the lion chasing the elephant .\",\n",
    "    \"I see the lion chasing the cat .\",\n",
    "    \"I see the lion chasing the koala .\",\n",
    "    \"I see the lion hunting the elephant .\",\n",
    "    \"I see the lion hunting the koala .\",\n",
    "    \"I see the tiger chasing the koala .\",\n",
    "    \"I see the tiger chasing the cat .\",\n",
    "    \"I see the tiger hunting the koala .\",\n",
    "]\n",
    "\n",
    "test_phrases = [\"I see the dog chasing the\", \"I see the tiger hunting the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_tokens = [token for sentence in raw_sentences for token in sentence.lower().split()]\n",
    "unique_tokens = sorted(list(set(all_tokens)))\n",
    "word_to_ix = {word: i for i, word in enumerate(unique_tokens)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "\n",
    "data = []\n",
    "for sentence in raw_sentences:\n",
    "    sentence_tokens = sentence.lower().split()\n",
    "    if len(sentence_tokens) >= CONTEXT_SIZE + 1:\n",
    "        for i in range(CONTEXT_SIZE, len(sentence_tokens)):\n",
    "            context = sentence_tokens[i - CONTEXT_SIZE:i]\n",
    "            target = sentence_tokens[i]\n",
    "            data.append((context, target))\n",
    "\n",
    "print(data)\n",
    "\n",
    "context_list = []\n",
    "target_list = []\n",
    "for context, target in data:\n",
    "    context_list.append([word_to_ix[w] for w in context])\n",
    "    target_list.append(word_to_ix[target])\n",
    "\n",
    "context_tensor = torch.tensor(context_list, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_list, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(context_tensor, target_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_flattened = embeds.view(inputs.size(0), -1)\n",
    "        hidden = self.activation(self.linear1(embeds_flattened))\n",
    "        output = self.linear2(hidden)\n",
    "        log_probs = torch.nn.functional.log_softmax(output, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = FeedForwardNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, CONTEXT_SIZE).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Training the model for {EPOCHS} epochs...\")\n",
    "loss_history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context_batch, target_batch in dataloader:\n",
    "        model.zero_grad()\n",
    "        context_batch, target_batch = context_batch.to(device), target_batch.to(device)\n",
    "        log_probs = model(context_batch)\n",
    "        loss = loss_function(log_probs, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_history.append(total_loss)\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Época {epoch + 1} - Pérdida: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_next_word(model, phrase):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        phrase_tokens = phrase.lower().split()\n",
    "        context_tokens = phrase_tokens[-CONTEXT_SIZE:]\n",
    "\n",
    "        print(context_tokens)\n",
    "\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context_tokens], dtype=torch.long).to(device)\n",
    "        context_idxs = context_idxs.unsqueeze(0)\n",
    "\n",
    "        log_probs = model(context_idxs)\n",
    "        probs = torch.exp(log_probs)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probs, 3)\n",
    "\n",
    "        top_words = [ix_to_word[i.item()] for i in top_indices[0]]\n",
    "\n",
    "        print(f\"Prediction for '{phrase}':\")\n",
    "        for i, word in enumerate(top_words):\n",
    "            print(f\"  {i+1}. '{word}' (Probability: {top_probs[0][i].item():.4f})\")\n",
    "\n",
    "print(\"\\n--- Predictions ---\")\n",
    "for phrase in test_phrases:\n",
    "    predict_next_word(model, phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Embeddings Visualization ---\")\n",
    "def visualize_embeddings(model, word_to_ix):\n",
    "    embeddings = model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "    if embeddings.shape[1] > 2:\n",
    "        print(\"Projecting embeddings to 2D using PCA...\")\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "    else:\n",
    "        embeddings_2d = embeddings\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for word, i in word_to_ix.items():\n",
    "        x, y = embeddings_2d[i]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(word, (x, y), textcoords=\"offset points\", xytext=(5,5), ha='center')\n",
    "\n",
    "    plt.title(\"Learned Embeddings Visualization\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_embeddings(model, word_to_ix)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
