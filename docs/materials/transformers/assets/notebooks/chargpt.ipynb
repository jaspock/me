{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Char-level language model based on transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/chargpt.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio PÃ©rez in 2024.\n",
        "\n",
        "This notebook presents \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "Generate text with a GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system:\n",
            "    seed: 3407\n",
            "    work_dir: ./out/chargpt\n",
            "data:\n",
            "    block_size: 128\n",
            "model:\n",
            "    model_type: gpt-mini\n",
            "    n_layer: None\n",
            "    n_head: None\n",
            "    n_embd: None\n",
            "    vocab_size: None\n",
            "    block_size: None\n",
            "    embd_pdrop: 0.1\n",
            "    resid_pdrop: 0.1\n",
            "    attn_pdrop: 0.1\n",
            "trainer:\n",
            "    device: auto\n",
            "    num_workers: 4\n",
            "    max_iters: None\n",
            "    batch_size: 64\n",
            "    learning_rate: 0.0005\n",
            "    betas: (0.9, 0.95)\n",
            "    weight_decay: 0.1\n",
            "    grad_norm_clip: 1.0\n",
            "\n",
            "data has 1115394 characters, 65 unique.\n",
            "number of parameters: 2.71M\n",
            "running on device cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 4.18417\n",
            "O God, O God!ry o e  ndrastt orsouoir te  mmmiou sno treeaeonou oyiiy  t m errss  mmn oo in syeotnto ae?e  mytm tr oy oooomroo  ia etne sost i eaoraoeote ynnto  one mnoters atsae orses  t am m o yr sesmt ort it ny tsmo  mor so ydnsastnme eato s oe ee se eee e tth oentrote oo  teoro t t et nAyd t s n ntoneeoe t my athsytnemme oti ia nnst onnteny ts  tsaimtoot s ormoryoeamyth et tyt   is  eramr    oonAysre nsmen  eoaety ar  omy  o o or emyd  a  too ens anee os tonAyeet aso st o sn tor it n oayt mr t onotmyit s\n",
            "saving model\n",
            "iter_dt 3778.35ms; iter 10: train loss 3.02733\n",
            "iter_dt 3562.02ms; iter 20: train loss 2.74399\n",
            "iter_dt 3583.76ms; iter 30: train loss 2.63549\n",
            "iter_dt 3021.95ms; iter 40: train loss 2.57377\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 676\u001b[0m\n\u001b[1;32m    673\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# run the optimization\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 526\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m    525\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# utils.py\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from ast import literal_eval\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def setup_logging(config):\n",
        "    \"\"\" monotonous bookkeeping \"\"\"\n",
        "    work_dir = config.system.work_dir\n",
        "    # create the work directory if it doesn't already exist\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    # log the args (if any)\n",
        "    with open(os.path.join(work_dir, 'args.txt'), 'w') as f:\n",
        "        f.write(' '.join(sys.argv))\n",
        "    # log the config itself\n",
        "    with open(os.path.join(work_dir, 'config.json'), 'w') as f:\n",
        "        f.write(json.dumps(config.to_dict(), indent=4))\n",
        "\n",
        "class CfgNode:\n",
        "    \"\"\" a lightweight configuration class inspired by yacs \"\"\"\n",
        "    # TODO: convert to subclass from a dict like in yacs?\n",
        "    # TODO: implement freezing to prevent shooting of own foot\n",
        "    # TODO: additional existence/override checks when reading/writing params?\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self._str_helper(0)\n",
        "\n",
        "    def _str_helper(self, indent):\n",
        "        \"\"\" need to have a helper to support nested indentation for pretty printing \"\"\"\n",
        "        parts = []\n",
        "        for k, v in self.__dict__.items():\n",
        "            if isinstance(v, CfgNode):\n",
        "                parts.append(\"%s:\\n\" % k)\n",
        "                parts.append(v._str_helper(indent + 1))\n",
        "            else:\n",
        "                parts.append(\"%s: %s\\n\" % (k, v))\n",
        "        parts = [' ' * (indent * 4) + p for p in parts]\n",
        "        return \"\".join(parts)\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\" return a dict representation of the config \"\"\"\n",
        "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
        "\n",
        "    def merge_from_dict(self, d):\n",
        "        self.__dict__.update(d)\n",
        "\n",
        "    def merge_from_args(self, args):\n",
        "        \"\"\"\n",
        "        update the configuration from a list of strings that is expected\n",
        "        to come from the command line, i.e. sys.argv[1:].\n",
        "\n",
        "        The arguments are expected to be in the form of `--arg=value`, and\n",
        "        the arg can use . to denote nested sub-attributes. Example:\n",
        "\n",
        "        --model.n_layer=10 --trainer.batch_size=32\n",
        "        \"\"\"\n",
        "        for arg in args:\n",
        "\n",
        "            keyval = arg.split('=')\n",
        "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
        "            key, val = keyval # unpack\n",
        "\n",
        "            # first translate val into a python object\n",
        "            try:\n",
        "                val = literal_eval(val)\n",
        "                \"\"\"\n",
        "                need some explanation here.\n",
        "                - if val is simply a string, literal_eval will throw a ValueError\n",
        "                - if val represents a thing (like an 3, 3.14, [1,2,3], False, None, etc.) it will get created\n",
        "                \"\"\"\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            # find the appropriate object to insert the attribute into\n",
        "            assert key[:2] == '--'\n",
        "            key = key[2:] # strip the '--'\n",
        "            keys = key.split('.')\n",
        "            obj = self\n",
        "            for k in keys[:-1]:\n",
        "                obj = getattr(obj, k)\n",
        "            leaf_key = keys[-1]\n",
        "\n",
        "            # ensure that this attribute exists\n",
        "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
        "\n",
        "            # overwrite the attribute\n",
        "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
        "            setattr(obj, leaf_key, val)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# mingpt.py\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "Full definition of a GPT Language Model, all of it in this single file.\n",
        "\n",
        "References:\n",
        "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
        "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
        "2) huggingface/transformers PyTorch implementation:\n",
        "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# from mingpt.utils import CfgNode as CfgNode\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
        "        C.model_type = 'gpt'\n",
        "        C.n_layer = None\n",
        "        C.n_head = None\n",
        "        C.n_embd =  None\n",
        "        # these options must be filled in externally\n",
        "        C.vocab_size = None\n",
        "        C.block_size = None\n",
        "        # dropout hyperparameters\n",
        "        C.embd_pdrop = 0.1\n",
        "        C.resid_pdrop = 0.1\n",
        "        C.attn_pdrop = 0.1\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        type_given = config.model_type is not None\n",
        "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        assert type_given ^ params_given # exactly one of these (XOR)\n",
        "        if type_given:\n",
        "            # translate from model_type to detailed configuration\n",
        "            config.merge_from_dict({\n",
        "                # names follow the huggingface naming conventions\n",
        "                # GPT-1\n",
        "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
        "                # GPT-2 configs\n",
        "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "                # Gophers\n",
        "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
        "                # (there are a number more...)\n",
        "                # I made these tiny models up\n",
        "                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
        "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
        "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
        "            }[config.model_type])\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Initialize a pretrained GPT model by copying over the weights\n",
        "        from a huggingface/transformers checkpoint.\n",
        "        \"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = cls.get_default_config()\n",
        "        config.model_type = model_type\n",
        "        config.vocab_size = 50257 # openai's model vocabulary\n",
        "        config.block_size = 1024  # openai's model block_size\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] # ignore these\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla nn.Linear.\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(keys) == len(sd)\n",
        "        for k in keys:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                # random note: because named_modules and named_parameters are recursive\n",
        "                # we will see the same tensors p many many times. but doing it this way\n",
        "                # allows us to know which parent module any tensor p belongs to...\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# trainer.py\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
        "so nothing in this file really has anything to do with GPT specifically.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# from mingpt.utils import CfgNode as CfgNode\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        # device to train on\n",
        "        C.device = 'auto'\n",
        "        # dataloder parameters\n",
        "        C.num_workers = 4\n",
        "        # optimizer parameters\n",
        "        C.max_iters = None\n",
        "        C.batch_size = 64\n",
        "        C.learning_rate = 3e-4\n",
        "        C.betas = (0.9, 0.95)\n",
        "        C.weight_decay = 0.1 # only applied on matmul weights\n",
        "        C.grad_norm_clip = 1.0\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "\n",
        "        # determine the device we'll train on\n",
        "        if config.device == 'auto':\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        else:\n",
        "            self.device = config.device\n",
        "        self.model = self.model.to(self.device)\n",
        "        print(\"running on device\", self.device)\n",
        "\n",
        "        # variables that will be assigned to trainer class later for logging and etc\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "\n",
        "        # setup the optimizer\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "\n",
        "        # setup the dataloader\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=config.num_workers,\n",
        "        )\n",
        "\n",
        "        model.train()\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = time.time()\n",
        "        data_iter = iter(train_loader)\n",
        "        while True:\n",
        "\n",
        "            # fetch the next batch (x, y) and re-init iterator if needed\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                batch = next(data_iter)\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            x, y = batch\n",
        "\n",
        "            # forward the model\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.trigger_callbacks('on_batch_end')\n",
        "            self.iter_num += 1\n",
        "            tnow = time.time()\n",
        "            self.iter_dt = tnow - self.iter_time\n",
        "            self.iter_time = tnow\n",
        "\n",
        "            # termination conditions\n",
        "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# chargpt.py\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "Trains a character-level language model.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# from mingpt.model import GPT\n",
        "# from mingpt.trainer import Trainer\n",
        "# from mingpt.utils import set_seed, setup_logging, CfgNode as CfgNode\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def get_config():\n",
        "\n",
        "    C = CfgNode()\n",
        "\n",
        "    # system\n",
        "    C.system = CfgNode()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out/chargpt'\n",
        "\n",
        "    # data\n",
        "    C.data = CharDataset.get_default_config()\n",
        "\n",
        "    # model\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-mini'\n",
        "\n",
        "    # trainer\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "    return C\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CfgNode()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# get default config and overrides from the command line, if any\n",
        "config = get_config()\n",
        "# config.merge_from_args(sys.argv[1:])\n",
        "print(config)\n",
        "setup_logging(config)\n",
        "set_seed(config.system.seed)\n",
        "\n",
        "# construct the training dataset\n",
        "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(config.data, text)\n",
        "\n",
        "# construct the model\n",
        "config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "config.model.block_size = train_dataset.get_block_size()\n",
        "model = GPT(config.model)\n",
        "\n",
        "# construct the trainer object\n",
        "trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "# iteration callback\n",
        "def batch_end_callback(trainer):\n",
        "\n",
        "    if trainer.iter_num % 10 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "    if trainer.iter_num % 500 == 0:\n",
        "        # evaluate both the train and test score\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # sample from the model...\n",
        "            context = \"O God, O God!\"\n",
        "            x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "            y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
        "            completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "            print(completion)\n",
        "        # save the latest model\n",
        "        print(\"saving model\")\n",
        "        ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        # revert model to training mode\n",
        "        model.train()\n",
        "\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "# run the optimization\n",
        "trainer.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
