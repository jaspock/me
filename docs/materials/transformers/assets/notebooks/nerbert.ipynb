{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Named entities recognition with transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Notebook and code written by Juan Antonio Pérez in 2023–2024.\n",
        "\n",
        "This notebook uses the encoder-like transformer of our previous notebook to train and test a toy-like named entity recognition model from a tiny dataset. \n",
        "\n",
        "It is assumed that you are already familiar with the basics of PyTorch. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "\n",
        "def make_batch(input_sentences, output_tags, word_index, tag_index, max_len, batch_size, device):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    data_cycle = itertools.cycle(zip(input_sentences, output_tags))\n",
        "\n",
        "    # to-do: adjust T to be minimum of the actual max length of the batch or max_len\n",
        "\n",
        "    while True:\n",
        "        for s,t in data_cycle:\n",
        "            words = s.split()\n",
        "            tags = t.split()\n",
        "            assert len(words) == len(tags)\n",
        "            inputs = [word_index[n] for n in words]\n",
        "            inputs = inputs + [0] * (max_len - len(inputs))  # padded inputs\n",
        "            tags = [tag_index[n] for n in tags]\n",
        "            tags = tags + [0] * (max_len - len(tags))  # padded outputs\n",
        "            input_batch.append(inputs)\n",
        "            output_batch.append(tags)\n",
        "\n",
        "            if len(input_batch) == batch_size:\n",
        "                yield torch.LongTensor(input_batch, device=device), torch.LongTensor(output_batch, device=device)\n",
        "                input_batch = []\n",
        "                output_batch = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code\n",
        "\n",
        "We load the `EncoderTransformer` class implemented in the previous notebook. If we are running this on the cloud, we download the file from GitHub. If we are running it locally, we assume that the file is in the same directory as this notebook. The seed is also set to a fixed value to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))  # running in Google Colab?\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input vocab size: 31\n",
            "output vocab size: 11\n"
          ]
        }
      ],
      "source": [
        "input_sentences = [\n",
        "    \"The cat sat on the mat .\",\n",
        "    \"I love eating pizza .\",\n",
        "    \"John is running in the park .\",\n",
        "    \"She gave him a beautiful gift .\",\n",
        "    \"They are playing soccer together .\",\n",
        "    \"The cat is eating pizza in the park .\"\n",
        "]\n",
        "\n",
        "output_tags = [\n",
        "    \"DET NOUN VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB VERB NOUN PUNCT\",\n",
        "    \"PROPN AUX VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB PRON DET ADJ NOUN PUNCT\",\n",
        "    \"PRON AUX VERB NOUN ADV PUNCT\",\n",
        "    \"DET NOUN AUX VERB NOUN ADP DET NOUN PUNCT\"\n",
        "]\n",
        "\n",
        "word_list = list(set(\" \".join(input_sentences).split()))\n",
        "word_index = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "special_tokens = len(word_index) \n",
        "for step, w in enumerate(word_list):\n",
        "    word_index[w] = step + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "input_vocab_size = len(word_index)\n",
        "tag_list = list(set(\" \".join(output_tags).split()))\n",
        "tag_index = {'[PAD]': 0}  # padding index must be 0\n",
        "for step, t in enumerate(tag_list):\n",
        "    tag_index[t] = step + 1\n",
        "index_tag = {i:t for i, t in enumerate(tag_index)}\n",
        "output_vocab_size = len(tag_index)\n",
        "print(\"input vocab size: %d\" % input_vocab_size)\n",
        "print(\"output vocab size: %d\" % output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.10M\n",
            "Step [0/1000], loss: 2.3123\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step [100/1000], loss: 0.0666\n",
            "Step [200/1000], loss: 0.0203\n",
            "Step [300/1000], loss: 0.0103\n",
            "Step [400/1000], loss: 0.0065\n",
            "Step [500/1000], loss: 0.0046\n",
            "Step [600/1000], loss: 0.0032\n",
            "Step [700/1000], loss: 0.0027\n",
            "Step [800/1000], loss: 0.0023\n",
            "Step [900/1000], loss: 0.0018\n",
            "Step [1000/1000], loss: 0.0019\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 3\n",
        "max_len = 12\n",
        "lr = 0.001\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = EncoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, input_vocab_size=input_vocab_size, output_vocab_size=output_vocab_size, \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "for inputs, outputs in make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, \n",
        "                                    tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if step % eval_steps == 0:\n",
        "        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break\n",
        "\n",
        "print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[19, 24, 18, 11,  4,  5, 13,  0,  0,  0,  0,  0],\n",
            "        [25,  9, 16, 10, 13,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [26, 22, 17, 23,  4, 20, 13,  0,  0,  0,  0,  0]]) tensor([[ 7, 10,  1,  3,  7, 10,  2,  0,  0,  0,  0,  0],\n",
            "        [ 6,  1,  1, 10,  2,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 4,  5,  1,  3,  7, 10,  2,  0,  0,  0,  0,  0]])\n",
            "Accuracy: 100.00%\n",
            "Input:\n",
            " The cat sat on the mat . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "I love eating pizza . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "John is running in the park . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Prediction: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT NOUN PUNCT ADP PUNCT PUNCT\n",
            "PRON VERB VERB NOUN PUNCT NOUN PUNCT NOUN PUNCT NOUN PUNCT PUNCT\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT NOUN PUNCT ADP PUNCT PUNCT\n",
            "Target: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PRON VERB VERB NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "# predict tags\n",
        "model.eval()\n",
        "inputs, outputs = make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device).__next__()\n",
        "print(inputs)\n",
        "print(outputs)\n",
        "logits = model(inputs)\n",
        "_, indices = torch.max(logits, dim=-1)\n",
        "# compute accuracy excluding pads:\n",
        "accuracy = torch.sum(indices[outputs!=0]==outputs[outputs!=0]).item()/torch.sum(outputs!=0).item()\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "predict_tags, true_tags, input_words = [], [], []  # 3 lists are required, not one\n",
        "for step in range(batch_size):\n",
        "    predict_tags.append(\" \".join([index_tag[each.item()] for each in indices[step]]))\n",
        "    true_tags.append(\" \".join([index_tag[each.item()] for each in outputs[step]]))\n",
        "    input_words.append(\" \".join([index_word[each.item()] for each in inputs[step]]))\n",
        "print(\"Input:\\n\", \"\\n\".join(input_words))\n",
        "print(\"Prediction: \\n\", \"\\n\".join(predict_tags))\n",
        "print(\"Target: \\n\", \"\\n\".join(true_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
        "\n",
        "✎ Compare the original pre-norm implementation of the transformer with the post-norm implementation under this task.\n",
        "\n",
        "✎ Add a pre-training step to the model that implements the masked language model objective and is trained on a separate corpus. Note that the `MASK` token is already included in the vocabulary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
