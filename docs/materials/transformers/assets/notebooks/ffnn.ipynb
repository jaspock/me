{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thlU7qzxespH"
   },
   "source": [
    "# Simple language model based on feed-forward neural networks\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/ffnn.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
    "\n",
    "Notebook and code written by Juan Antonio PÃ©rez in 2024. Originally inspired by the Tae Hwan Jung's code (@graykode) at the [NLP tutorial](https://github.com/graykode/nlp-tutorial).\n",
    "\n",
    "This notebook presents an intentionally simple language model based on feed-forward neural networks. It is so oversimplified that it will be trained with sentences of exactly three words and will only be able to predict the last word of a sentence given all the previous words. Moreover, our training set will be so small that the model will probably memorize it. This implies that the network will not be able to generalize to sentences that it has not seen during training, and, as a result of that, we will test it with the same sentences used for training; this constitutes a very bad practice, that we will only use here for the sake of simplicity. In spite of all these limitations, this notebook will be useful to illustrate the basic concepts of language modeling with simple neural networks and correctly implement the architecture presented in the book by Jurafsky and Martin.\n",
    "\n",
    "It is assumed that you are already familiar with the basics of PyTorch, but at a absolute beginner level only. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "set_seed(42)  # to ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data and vocabulary\n",
    "\n",
    "The following code defines the training data and the vocabulary. The global variable `window_size` defines the number of words that will be used to predict the next word. You probably understand every line of code at this point, as all is quite similar to what we have seen in previous notebooks. The function `make_batch` generates a batch of data containing the whole training set. Remember that mini-batches usually contain a subset of the training set, but we oversimplify things here and use the whole training set as a single batch. The function returns input and target data as lists of integers, each integer representing a word index in the vocabulary. These lists are obtained by iterating over the training set and extracting subsequences of `window_size` words as input and the next word as target. When converted to tensor later, the input will be of shape `(batch_size, window_size)` and the target of shape `(batch_size, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "\n",
    "sentences = [\"let's promote peace\", \"let's ban war\", \"let's teach compassion\", \"let's build a better world\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_index = {w: i for i, w in enumerate(word_list)}\n",
    "index_word = {i: w for i, w in enumerate(word_list)}\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "print(f\"word_index = {index_word}\")\n",
    "\n",
    "def make_batch():\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # space tokenizer\n",
    "        for i in range(len(words) - 2):\n",
    "            input = [word_index[words[i]], word_index[words[i+1]]]  # indices of two consecutive words\n",
    "            target = word_index[words[i+2]]  # index of the next word\n",
    "            input_batch.append(input)\n",
    "            target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "inputs, targets = make_batch()\n",
    "print(f\"inputs = {inputs}\")\n",
    "print(f\"targets = {targets}\")\n",
    "print(f\"inputs = {[[index_word[i] for i in x] for x in inputs]}\")\n",
    "print(f\"targets = {[index_word[i] for i in targets]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Our model parameters are based on already known PyTorch classes. The embedding layer is a `torch.nn.Embedding` object, and the transition from the input to the hidden layer is performed through a `torch.nn.Linear` object (inside, a weight matrix and a bias vector). The activation function at the hidden layer is computed at the `forward` function using the `nn.functional.relu` function. Note that as the ReLU function does not have learnable parameters, we do not need to declare an object of the `torch.nn.ReLU` class in the constructor. Finally, the transition from the hidden to the output layer could be performed through another `torch.nn.Linear` object. Note that the book does not use a bias vector at the output layer (the model would behave similarly with or without it, anyway), so we would set the `bias` parameter to `False` when creating this second `torch.nn.Linear` object. \n",
    "\n",
    "Although, we could have defined this second object of class `torch.nn.Linear` in the constructor, we have adopted a different approach here just to show how you can easily add learnable parameters of any type to your model. Objects of class `torch.nn.Parameter` represent tensors with learnable weights. Unlike a regular tensor that you could create in the constructor, it will be automatically added to the list of parameters of the model and will be returned when calling the function `torch.model.parameters`.  We initialize this as a random tensor of size `(hidden_size, vocab_size)` representing the matrix $\\mathbf{U}$ in the book.\n",
    "\n",
    "As already seen, the first dimension in all the operations in the `forward` function corresponds to the different items in the batch. Remember that the input to the network is a batch of lists of indices, so the first dimension of the input tensor will be the batch size. The second dimension of the input tensor is `window_size`, as we are feeding the network with the indices of `window_size` words. The embedding layer will return a tensor of size `(batch_size, window_size, embedding_size)`, where `embedding_size` is the size of each of the embedding vectors. The `torch.Tensor.view` function allows us to reshape the input tensor to `(batch_size, window_size * embedding_size)`, which is the expected size for the input layer. This concatenates the embedding vectors of the input words. The `-1`  in the first argument means that the size of the first dimension is inferred from the size of the arguments so that the total number of elements remains the same. In our case, it is an elegant way to avoid hardcoding the batch size or even using `X.shape[0]` to get it. The hidden layer will then return a tensor of size `(batch_size, hidden_size)`, and the output layer will return a tensor of size `(batch_size, vocab_size)`.\n",
    "\n",
    "Finally, observe that the `forward` function returns the so called *logits*, that is, the output of the model before applying normalization through the softmax function. This is a common practice in PyTorch, as it allows to apply the softmax function only when needed. For example, if we are interested in the most likely word in the prediction, we may simply apply the `torch.argmax` function to the output of the model and avoid the computation of the softmax. In addition to this, the fact that the unnormalized logits are returned conditions the loss function that we will use to train the model. The `torch.nn.CrossEntropyLoss` loss function that we will use later assumes these unnormalized logits as input.\n",
    "\n",
    "The multiplication of one-hot vectors by the embedding matrix discussed in the Jurafsky and Martin's book is not necessary, as we have a straightforward way to obtain the embedding of a word from an index by using `torch.nn.Embedding`.\n",
    "\n",
    "ðŸ“˜ *Documentation:* [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), [torch.nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.Parameter.html), [torch.nn.functional.relu](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html), [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html), [torch.Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html), [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html), [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, window_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.C = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.W = nn.Linear(window_size * embedding_size, hidden_size, bias=True)\n",
    "        self.U = nn.Parameter(torch.randn(hidden_size, vocab_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.C(X) \n",
    "        X = X.view(-1, self.window_size * self.embedding_size)\n",
    "        X = F.relu(self.W(X))\n",
    "        X = torch.matmul(X, self.U) \n",
    "        return X  # return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "The main novelty here is the use of the `torch.optim.lr_scheduler.LinearLR` class to implement a linear learning rate decay. The `torch.optim.lr_scheduler` module provides several classes to implement different learning rate evolution strategies. The `LinearLR` class implements a linear decay from an initial learning rate to a final learning rate. The initial learning rate is set to `lr` in the constructor, and the final learning rate is set to `lr * end_factor`. The `end_factor` parameter is set to 0.5 in the code above, so the final learning rate will be half the initial learning rate. The `total_iters` parameter is used to compute the learning rate at each step. The learning rate at step `i` is computed as `lr * (1 - i / total_iters)`. The `get_last_lr` function returns the learning rate at the last step. The `torch.optim.lr_scheduler.step` function is called at each training step to update the learning rate, just after the optimizer step. We use the learning rate scheduler in combination with the Adam optimizer, but you can use it with any optimizer.\n",
    "\n",
    "ðŸ“˜ *Documentation:* [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), [torch.optim.lr_scheduler.LinearLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html), [torch.optim.lr_scheduler.get_last_lr](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.get_last_lr.html), [torch.optim.lr_scheduler.step](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.step.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "hidden_size = 4 \n",
    "embedding_size = 2\n",
    "training_steps = 1000\n",
    "eval_steps = 100\n",
    "lr = 0.005\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = NNLM(vocab_size, embedding_size, window_size, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
    "\n",
    "input_batch, target_batch = make_batch()\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "lr_history = []  # learning rate history\n",
    "model.train()\n",
    "for i in range(training_steps):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if i % eval_steps == 0:\n",
    "        print(f'Step [{i}/{training_steps}], loss: {loss.item():.4f}')\n",
    "    lr_history.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()  # scheduler must be called after optimizer\n",
    "\n",
    "print(f'Step [{training_steps}/{training_steps}], loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the evolution of the base learning rate\n",
    "\n",
    "Our learning rate scheduler is quite simple, but we plot it anyway to see that it evolves as expected during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lr_history)\n",
    "plt.title(\"Evolution of learning rate during training\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the model with its own training data, which is a terrible practice, but the only one that makes some sense giving the extremely small size of the training set that will not allow the model to generalize to unseen data or develop emergent behaviors. \n",
    "\n",
    "ðŸ“˜ *Documentation:*  [torch.Tensor.argmax](https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html), [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict = model(input_batch).argmax(dim=1)\n",
    "    # iterate through input_batch and print the words corresponding to the indices in every row, then print the corresponding row in predict:\n",
    "    for i in range(len(input_batch)):\n",
    "        print([index_word[n.item()] for n in input_batch[i]], 'âŸ¶', index_word[predict[i].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
    "\n",
    "âœŽ Replace the scheduler with one that uses the `LambdaLR` scheduler of PyTorch. Implement a lambda function that increases the learning rate for the initial `warmup_steps`, then decreases it inversely proportional to the square root of the step count. This a common learning rate scheduler used in transformer models.\n",
    "\n",
    "âœŽ Comment out the scheduler and train the model without it. Perform a number of training runs with different random seeds and compare the results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
