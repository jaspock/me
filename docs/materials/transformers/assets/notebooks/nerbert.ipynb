{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Named entities recognition with transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio PÃ©rez in 2024.\n",
        "\n",
        "This notebook presents \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input vocab size: 31\n",
            "output vocab size: 11\n",
            "number of parameters: 0.10M\n",
            "Step     1, loss 2.435829\n",
            "Step   100, loss 1.065557\n",
            "Step   200, loss 0.180464\n",
            "Step   300, loss 0.043454\n",
            "Step   400, loss 0.019343\n",
            "Step   500, loss 0.012184\n",
            "Step   600, loss 0.009387\n",
            "Step   700, loss 0.007273\n",
            "Step   800, loss 0.006261\n",
            "Step   900, loss 0.006185\n",
            "Step  1000, loss 0.006055\n",
            "tensor([[ 5, 14, 19, 12, 15, 29, 10,  0,  0,  0,  0,  0],\n",
            "        [26, 27, 21,  4, 10,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [28, 22, 25, 20, 15, 24, 10,  0,  0,  0,  0,  0]]) tensor([[ 1, 10,  6,  8,  1, 10,  9,  0,  0,  0,  0,  0],\n",
            "        [ 7,  6,  6, 10,  9,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 3,  4,  6,  8,  1, 10,  9,  0,  0,  0,  0,  0]])\n",
            "Input:\n",
            " The cat sat on the mat . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "I love eating pizza . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "John is running in the park . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Prediction: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT NOUN NOUN NOUN NOUN VERB\n",
            "PRON VERB VERB NOUN PUNCT NOUN VERB NOUN VERB NOUN VERB VERB\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT NOUN PUNCT NOUN VERB VERB\n",
            "Target: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PRON VERB VERB NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Original code from minGPT by Andrej Karpathy\n",
        "# https://github.com/karpathy/minGPT/\n",
        "# Modifications by @jaspock\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "import itertools\n",
        "\n",
        "use_simple_attention = True\n",
        "\n",
        "def make_batch(input_sentences, output_tags, word_index, tag_index, max_len, batch_size, device):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    data_cycle = itertools.cycle(zip(input_sentences, output_tags))\n",
        "\n",
        "    # to-do: adjust T to be minimum of the actual max length of the batch or max_len\n",
        "\n",
        "    while True:\n",
        "        for s,t in data_cycle:\n",
        "            words = s.split()\n",
        "            tags = t.split()\n",
        "            assert len(words) == len(tags)\n",
        "            inputs = [word_index[n] for n in words]\n",
        "            inputs = inputs + [0] * (max_len - len(inputs))  # padded inputs\n",
        "            tags = [tag_index[n] for n in tags]\n",
        "            tags = tags + [0] * (max_len - len(tags))  # padded outputs\n",
        "            input_batch.append(inputs)\n",
        "            output_batch.append(tags)\n",
        "\n",
        "            if len(input_batch) == batch_size:\n",
        "                yield torch.LongTensor(input_batch, device=device), torch.LongTensor(output_batch, device=device)\n",
        "                input_batch = []\n",
        "                output_batch = []\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    # to-do: check that this is equivalent to the PyTorch implementation and replace nn.LayerNorm with it\n",
        "    # to-do: check the init_weights function\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "class HeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_embd_head, attn_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.q_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        self.k_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        self.v_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, x, padding_mask): \n",
        "        q = self.q_lin(x) # (B, T, n_embd) -> (B, T, n_embd_head)\n",
        "        k = self.k_lin(x)\n",
        "        v = self.v_lin(x)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # expand mask so that broadcasting between (B, T, T) and (B, 1, T) applies\n",
        "        expanded_mask = padding_mask.unsqueeze(1) # (T, T) -> (B, 1, T)\n",
        "        att = att.masked_fill(expanded_mask, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        return att @ v # (B, T, T) @ (B, T, n_embd_head) -> (B, T, n_embd_head)\n",
        "\n",
        "\n",
        "class MultiHeadAttentionSimple(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads as a list\n",
        "        self.heads = nn.ModuleList([HeadAttention(n_embd, n_embd // n_head, attn_pdrop) for _ in range(n_head)])\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)  # output projection to integrate head outputs\n",
        "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        y = torch.cat([h(x, padding_mask) for h in self.heads], dim=-1)  # (B, T, n_embd)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MultiHeadAttentionOriginal(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "        # to-do: check that this class behaves similar to the simpler one\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        mask = padding_mask.view(1,1,T,T)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(mask[:,:,:T,:T] == 1, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        if not use_simple_attention: # original code\n",
        "            self.attn = MultiHeadAttentionOriginal(n_embd, n_head, attn_pdrop, resid_pdrop)\n",
        "        else:\n",
        "            self.attn = MultiHeadAttentionSimple(n_embd, n_head, attn_pdrop, resid_pdrop)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(n_embd, 4 * n_embd),  # ffw hidden layer size is fixed to 4*n_embd\n",
        "            c_proj  = nn.Linear(4 * n_embd, n_embd),\n",
        "            act     = nn.GELU(),\n",
        "            dropout = nn.Dropout(resid_pdrop),\n",
        "        ))\n",
        "        \n",
        "    def forward(self, x, padding_mask):\n",
        "        x = x + self.attn(self.ln_1(x),padding_mask)\n",
        "        m = self.mlp  # just a shorter name\n",
        "        x = x +  m.dropout(m.c_proj(m.act(m.c_fc(self.ln_2(x)))))\n",
        "        return x\n",
        "\n",
        "class EncoderTransformer(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, n_layer, input_vocab_size, output_vocab_size, max_len, \n",
        "                 embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(input_vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(max_len, n_embd),\n",
        "            drop = nn.Dropout(embd_pdrop),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, attn_pdrop, resid_pdrop) for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm(n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, output_vocab_size, bias=False)\n",
        "        self._init_weights()\n",
        "        \n",
        "        # report number of parameters (note we don't count the parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "                torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B, T = inputs.size()\n",
        "        device = inputs.device\n",
        "        mask = inputs == 0  # padding mask\n",
        "        mask.to(device)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "        tok_emb = self.transformer.wte(inputs)  # (B, T, C)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (1, T, C)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, mask)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 3\n",
        "max_len = 12\n",
        "lr = 0.001\n",
        "train_steps = 1000\n",
        "\n",
        "input_sentences = [\n",
        "    \"The cat sat on the mat .\",\n",
        "    \"I love eating pizza .\",\n",
        "    \"John is running in the park .\",\n",
        "    \"She gave him a beautiful gift .\",\n",
        "    \"They are playing soccer together .\",\n",
        "    \"The cat is eating pizza in the park .\"\n",
        "]\n",
        "\n",
        "output_tags = [\n",
        "    \"DET NOUN VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB VERB NOUN PUNCT\",\n",
        "    \"PROPN AUX VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB PRON DET ADJ NOUN PUNCT\",\n",
        "    \"PRON AUX VERB NOUN ADV PUNCT\",\n",
        "    \"DET NOUN AUX VERB NOUN ADP DET NOUN PUNCT\"\n",
        "]\n",
        "\n",
        "word_list = list(set(\" \".join(input_sentences).split()))\n",
        "word_index = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "special_tokens = len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "input_vocab_size = len(word_index)\n",
        "tag_list = list(set(\" \".join(output_tags).split()))\n",
        "tag_index = {'[PAD]': 0}  # padding index must be 0\n",
        "for i, t in enumerate(tag_list):\n",
        "    tag_index[t] = i + 1\n",
        "index_tag = {i:t for i, t in enumerate(tag_index)}\n",
        "output_vocab_size = len(tag_index)\n",
        "print(\"input vocab size: %d\" % input_vocab_size)\n",
        "print(\"output vocab size: %d\" % output_vocab_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = EncoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, input_vocab_size=input_vocab_size, output_vocab_size=output_vocab_size, \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=train_steps, epochs=1, anneal_strategy='cos')\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "for inputs, outputs in make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, \n",
        "                                    tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device):\n",
        "    padding_mask = inputs == 0\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    \n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if (step + 1) % 100 == 0 or step == 0:\n",
        "        print(f\"Step {(step + 1):5d}, loss {loss.item():.6f}\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==train_steps):\n",
        "        break\n",
        "\n",
        "# predict tags\n",
        "model.eval()\n",
        "inputs, outputs = make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device).__next__()\n",
        "print(inputs,outputs)\n",
        "logits = model(inputs)\n",
        "_, indices = torch.max(logits, dim=-1)\n",
        "predict_tags, true_tags, input_words = [], [], []  # 3 lists are required, not one\n",
        "for i in range(batch_size):\n",
        "    predict_tags.append(\" \".join([index_tag[each.item()] for each in indices[i]]))\n",
        "    true_tags.append(\" \".join([index_tag[each.item()] for each in outputs[i]]))\n",
        "    input_words.append(\" \".join([index_word[each.item()] for each in inputs[i]]))\n",
        "print(\"Input:\\n\", \"\\n\".join(input_words))\n",
        "print(\"Prediction: \\n\", \"\\n\".join(predict_tags))\n",
        "print(\"Target: \\n\", \"\\n\".join(true_tags))\n",
        "\n",
        "\n",
        "# %%\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
