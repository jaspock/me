{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Named entity recognition with transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Notebook and code written by Juan Antonio Pérez in 2023–2024.\n",
        "\n",
        "This notebook uses the encoder-like transformer of our previous notebook to train and test a toy-like named entity recognition (NER) model from a tiny dataset. NER consists of identifying and classifying named entities in texts into a number of pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "It is assumed that you are already familiar with the basics of PyTorch. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation\n",
        "\n",
        "The auxiliary function `sample_indexes` takes a sentence and its corresponding tags and returns a pair of lists of indexes. The first list contains the indexes of the words in the sentence, and the second list contains the indexes of the tags. The indexes are obtained from the dictionaries `word_index` and `tag_index`. The function also takes care of padding the lists to the maximum length `max_len` with the index `pad_index`.\n",
        "\n",
        "The function `make_batch` is another example of data generator. `itertools.cycle` is used to repeat the data indefinitely. It creates an iterator that returns elements from an iterable, saving a copy of each element. Once the iterable is exhausted, it starts returning elements from the saved copy. This means that it may require significant memory if the iterable is long. For real training data, it is usually much better to use a generator that reads the data from disk mini-batch by mini-batch.\n",
        "\n",
        "Our iterable is made of tuples of sentences and tags. Python's `zip` function creates an iterator that aggregates elements from each of the iterables. For example, `zip([1,2,3], [4,5,6])` returns `[(1,4), (2,5), (3,6)]`.\n",
        "\n",
        "It is not being done here, but note that the `PAD` token is so often represented by the index 0 that it is common to hardcode its value in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "\n",
        "def sample_indexes(sentence, tags, word_index, tag_index, max_len, pad_word_id, pad_tag_id):\n",
        "    words = sentence.split()\n",
        "    tags = tags.split()\n",
        "    assert len(words) == len(tags), \"Lengths of input sentences and labels do not match\"\n",
        "    # truncate lists to max_len:\n",
        "    if len(words) > max_len:\n",
        "        words = words[:max_len]\n",
        "        tags = tags[:max_len]\n",
        "    inputs = [word_index.get(n, word_index['[UNK]']) for n in words]\n",
        "    inputs = inputs + [pad_word_id] * (max_len - len(inputs))  # padded inputs\n",
        "    tags = [tag_index[n] for n in tags]\n",
        "    tags = tags + [pad_tag_id] * (max_len - len(tags))  # padded outputs\n",
        "    return inputs, tags\n",
        "\n",
        "def make_batch(input_sentences, output_tags, word_index, tag_index, max_len, batch_size, pad_word_id, pad_tag_id, device):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    data_cycle = itertools.cycle(zip(input_sentences, output_tags))\n",
        "    while True:\n",
        "        for s,t in data_cycle:\n",
        "            inputs, outputs = sample_indexes(s, t, word_index, tag_index, max_len, pad_word_id, pad_tag_id)\n",
        "            input_batch.append(inputs)\n",
        "            output_batch.append(outputs)\n",
        "            if len(input_batch) == batch_size:\n",
        "                yield torch.LongTensor(input_batch, device=device), torch.LongTensor(output_batch, device=device)\n",
        "                input_batch = []\n",
        "                output_batch = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code\n",
        "\n",
        "We load the `EncoderTransformer` class implemented in the previous notebook. If we are running this on the cloud, we download the file from GitHub. If we are running it locally, we assume that the file is in the same directory as this notebook. The seed is also set to a fixed value to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))  # running in Google Colab?\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    !wget https://raw.githubusercontent.com/jaspock/me/main/docs/materials/transformers/assets/notebooks/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing\n",
        "\n",
        "This code does not add novel elements to what you have already seen in the previous notebook. Note that, in addition to `PAD`, we add some special tokens which will not be used in this notebook, but we leave them there for potential future use as they are common in NLP tasks based on encoders. The tags used here for named entity recognition are `PER` (person), `LOC` (location), `ORG` (organization), `MISC` (miscellaneous), and `O` (other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_sentences = [\n",
        "    \"Steve Jobs founded Apple in Cupertino .\",\n",
        "    \"The Eiffel Tower is located in Paris .\",\n",
        "    \"I am currently reading 1984 by George Orwell .\",\n",
        "    \"The United Nations was established in 1945 .\",\n",
        "    \"Mount Everest is the highest mountain in the world .\",\n",
        "    \"Shakespeare wrote Romeo and Juliet .\"\n",
        "]\n",
        "\n",
        "output_tags = [\n",
        "    \"PER PER O ORG O LOC O\",\n",
        "    \"O MISC MISC O O O LOC O\",\n",
        "    \"O O O O MISC O PER PER O\",\n",
        "    \"O ORG ORG O O O O O\",\n",
        "    \"LOC LOC O O O O O O LOC O\",\n",
        "    \"PER O PER O PER O\"\n",
        "]\n",
        "\n",
        "word_list = list(set(\" \".join(input_sentences).split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\n",
        "special_tokens = len(word_index) \n",
        "for step, w in enumerate(word_list):\n",
        "    word_index[w] = step + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "input_vocab_size = len(word_index)\n",
        "tag_list = list(set(\" \".join(output_tags).split()))\n",
        "tag_index = {'[PAD]': 0}\n",
        "for step, t in enumerate(tag_list):\n",
        "    tag_index[t] = step + 1\n",
        "index_tag = {i:t for i, t in enumerate(tag_index)}\n",
        "output_vocab_size = len(tag_index)\n",
        "print(f\"input_vocab_size = {input_vocab_size}\")\n",
        "print(f\"output_vocab_size = {output_vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training\n",
        "\n",
        "Hopefully, having studied the other notebooks, once you reach this point, you will realize that everything sounds familiar and understandable.\n",
        "\n",
        "The `for` loop automatically calls the `__next__` method of the iterator returned by `make_batch`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 3\n",
        "max_len = 12\n",
        "lr = 0.001\n",
        "training_steps = 500\n",
        "eval_steps = 100\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = EncoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, input_vocab_size=input_vocab_size, output_vocab_size=output_vocab_size, \n",
        "                max_len=max_len, pad_index = word_index['[PAD]'], embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "for inputs, outputs in make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, \n",
        "                                    tag_index=tag_index, max_len=max_len, batch_size=batch_size, \n",
        "                                    pad_word_id = word_index['[PAD]'], pad_tag_id = tag_index['[PAD]'], device=device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if step % eval_steps == 0:\n",
        "        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break\n",
        "\n",
        "print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\n",
        "\n",
        "We measure the accuracy of the model by comparing the predicted tags with the gold tags. As expected, we do not take into account the `PAD` tokens when computing the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "pad_word_id = word_index['[PAD]']\n",
        "pad_tag_id = tag_index['[PAD]']\n",
        "test_sentence = \"Steve Jobs wrote Romeo and Juliet .\"\n",
        "expected_tags = \"PER PER O PER O PER O\"\n",
        "inputs, outputs = sample_indexes(test_sentence, expected_tags, word_index, tag_index, max_len, pad_word_id, pad_tag_id)\n",
        "\n",
        "inputs = torch.LongTensor(inputs, device=device).unsqueeze(0)  # convert to batch of size 1\n",
        "outputs = torch.LongTensor(outputs, device=device).unsqueeze(0)\n",
        "logits = model(inputs)\n",
        "_, indices = torch.max(logits, dim=-1)\n",
        "\n",
        "# compute accuracy excluding pads:\n",
        "accuracy = torch.sum(indices[outputs!=pad_word_id]==outputs[outputs!=pad_word_id]).item()/torch.sum(outputs!=pad_word_id).item()\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "print()\n",
        "\n",
        "i = 0\n",
        "print(f'Input: {inputs[i]}')\n",
        "print(f'Expected output: {outputs[i]}')\n",
        "print(f'Predicted output: {indices[i]}')\n",
        "# print words and tags using index_word and index_tag:\n",
        "print(f'Input words: {[index_word[w.item()] for w in inputs[i] if w!=pad_word_id]}')\n",
        "print(f'Expected tags: {[index_tag[t.item()] for t in outputs[i] if t!=pad_tag_id]}')\n",
        "print(f'Predicted tags: {[index_tag[t.item()] for t in indices[i] if t!=pad_tag_id]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
        "\n",
        "✎ Compare the original pre-norm implementation of the transformer with the post-norm implementation under this task.\n",
        "\n",
        "✎ Add a pre-training step to the model that implements the masked language model objective and is trained on a separate corpus. Note that the `MASK` token is already included in the vocabulary.\n",
        "\n",
        "✎ Exclude some words in the training corpus from the vocabulary and check that they are replaced by the `UNK` token and that some representations are learned for it.\n",
        "\n",
        "✎ Modify the code so that predictions are not computed for the `PAD` tokens. Make sure your implementation works for mini-batches containing sentences of different lengths."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
