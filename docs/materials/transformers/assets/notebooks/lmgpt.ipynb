{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Named entities recognition with transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio Pérez in 2024.\n",
        "\n",
        "This notebook presents \n",
        "\n",
        "**Exercise**: use spm to tokenize the data \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nos ahorramos el padding en make_batch\n",
        "Comentar sobre desconocidas y UNK en train y test\n",
        "Hacer versión corta con MultiHeadAttention y LN de PyTorch\n",
        "to-do: check that original multiheadattention behaves similar to the simpler one\n",
        "mover transformer a notebook y poner un main que ejecuta uno muy pequeño inicializado aleatoriamente\n",
        "wget github y %run 'note.ipynb' para ejecutarlo en celda de notebook\n",
        "hacer una clase comun a encoder y decoder (solo cambia la mascara), TransformerModule, EncoderOnlyTranformer, DecoderOnlyTransformer\n",
        "poner set_seed en los otros notebooks y evaluarlo\n",
        "añadir a NER el lr scheduler\n",
        "Ejercicio: que funcione con minibatches en inferencia\n",
        "Repasar el main del BERT para hacerlo como aquí\n",
        "usar no_grad en todas la evaluaciones\n",
        "avisar que no usaremos prefix tuning o como se llame (supongo que hay que usarla tb durante el training)\n",
        "ejercicio: top-p sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 148\n",
            "number of parameters: 0.11M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step     1, loss nan\n",
            "tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "Step   100, loss nan\n",
            "tensor(nan, grad_fn=<NllLossBackward0>)\n",
            "Step   200, loss nan\n",
            "tensor(nan, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 252\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m    251\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 252\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    254\u001b[0m step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Original code from minGPT by Andrej Karpathy\n",
        "# https://github.com/karpathy/minGPT/\n",
        "# Modifications by @jaspock\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def make_language_modeling_batch(corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    tokens = corpus.split()\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokens]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    batch_token_length = batch_size * max_len  # the total number of tokens in a batch\n",
        "    assert n_tokens >= batch_token_length, f'Short corpus ({n_tokens} tokens), must be at least {batch_length} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is the same as input, except shifted one token to the right\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.a = nn.Parameter(torch.ones(size))\n",
        "        self.b = nn.Parameter(torch.zeros(size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a * (x - mean) / (std + self.eps) + self.b\n",
        "\n",
        "\n",
        "class HeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_embd_head, attn_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.q_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        self.k_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        self.v_lin = nn.Linear(n_embd, n_embd_head)\n",
        "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, x, mask): \n",
        "        B, T, C = x.size()  # batch size, sequence length, main embedding dim, C' = head embedding dim\n",
        "        q = self.q_lin(x)  # (B, T, C) -> (B, T, C')\n",
        "        k = self.k_lin(x)\n",
        "        v = self.v_lin(x)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        mask = mask.view(1,T,T) # expand mask, (T, T) -> (1, T, T)\n",
        "        att = att.masked_fill(mask, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        return att @ v  # (B, T, T) @ (B, T, C') -> (B, T, C')\n",
        "\n",
        "\n",
        "class MultiHeadAttentionSimple(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.heads = nn.ModuleList([HeadAttention(n_embd, n_embd // n_head, attn_pdrop) for _ in range(n_head)])\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)  # output projection to integrate head outputs\n",
        "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        y = torch.cat([h(x, padding_mask) for h in self.heads], dim=-1)  # [(B,T,C')] -> (B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MultiHeadAttentionEfficient(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(resid_pdrop)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.size() # batch size, sequence length, main embedding dim, C' = head embedding dim\n",
        "        H = self.n_head\n",
        "        Cp = C // H\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, H, T, C')\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, H, T, C')\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, H, T, C')\n",
        "\n",
        "        mask = mask.view(1,1,T,T)\n",
        "\n",
        "        # self-attention: (B, H, T, C') x (B, H, C', T) -> (B, H, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(mask[:,:,:T,:T] == 1, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, H, T, T) x (B, H, T, C') -> (B, H, T, C')\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "MultiHeadAttention = MultiHeadAttentionSimple\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, attn_pdrop, resid_pdrop):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head, attn_pdrop, resid_pdrop)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(n_embd, 4 * n_embd),  # ffw hidden layer size is fixed to 4*n_embd\n",
        "            c_proj  = nn.Linear(4 * n_embd, n_embd),\n",
        "            act     = nn.GELU(),\n",
        "            dropout = nn.Dropout(resid_pdrop),\n",
        "        ))\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x = x + self.attn(self.ln_1(x),mask)\n",
        "        x = x +  self.mlp.dropout(self.mlp.c_proj(self.mlp.act(self.mlp.c_fc(self.ln_2(x)))))\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, n_layer, vocab_size, max_len, \n",
        "                 embd_pdrop=0.1, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(max_len, n_embd),\n",
        "            drop = nn.Dropout(embd_pdrop),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, attn_pdrop, resid_pdrop) for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm(n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "        self._init_weights()\n",
        "        # report number of parameters (note we don't count the parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(f\"number of parameters: {(n_params/1e6):.2f}M\")\n",
        "        \n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            # elif isinstance(module, nn.LayerNorm):\n",
        "            #    torch.nn.init.zeros_(module.bias)\n",
        "            #    torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        B, T = inputs.size()\n",
        "        device = inputs.device\n",
        "        mask = torch.tril(torch.ones((T, T), device=device)).bool()  # causal attention mask            \n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "\n",
        "        tok_emb = self.transformer.wte(inputs)  # (B, T, C)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (1, T, C)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x, mask)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 4\n",
        "max_len = 32\n",
        "train_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.000001\n",
        "\n",
        "corpus = \"\"\"\n",
        "En un lugar de La Mancha de cuyo nombre no quiero acordarme no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero adarga antigua rocín \n",
        "flaco y galgo corredor Una olla de algo más vaca que carnero salpicón las más noches duelos y quebrantos los sábados lentejas los viernes algún palomino \n",
        "de añadidura los domingos consumían las tres partes de su hacienda El resto della concluían sayo de velarte calzas de velludo para las fiestas con sus \n",
        "pantuflos de lo mismo y los días de entresemana se honraba con su vellorí de lo más fino Tenía en su casa una ama que pasaba de los cuarenta y una \n",
        "sobrina que no llegaba a los veinte y un mozo de campo y plaza que así ensillaba el rocín como tomaba la podadera Frisaba la edad de nuestro hidalgo con \n",
        "los cincuenta años Era de complexión recia seco de carnes enjuto de rostro gran madrugador y amigo de la caza Quieren decir que tenía el sobrenombre de \n",
        "Quijada o Quesada que en esto hay alguna diferencia en los autores aunque por conjeturas verisímiles se deja entender que se llamaba Quijana Pero esto \n",
        "importa poco a nuestro cuento basta que en la narración dél no se salga un punto de la verdad del cual no pudiera poner en duda el más acreditado historiador\n",
        "\"\"\"\n",
        "\n",
        "word_list = list(set(corpus.split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1, '[EOS]': 2}\n",
        "special_tokens= len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "vocab_size = len(word_index)\n",
        "print(\"vocab size: %d\" % vocab_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,  \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=train_steps, epochs=1, anneal_strategy='cos')\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "\n",
        "for inputs, outputs in make_language_modeling_batch(corpus, word_index, max_len, batch_size, device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if (step + 1) % eval_steps == 0 or step == 0:\n",
        "        print(f\"Step {(step + 1):5d}, loss {loss.item():.6f}\")\n",
        "        print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==train_steps):\n",
        "        break\n",
        "\n",
        "def generate_text(model, prompt, word_index, index_word, max_len, device):\n",
        "    words = prompt.split()\n",
        "    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n",
        "    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(input_ids)):\n",
        "            output = model(input)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n",
        "            predicted_word = index_word[predicted_id]\n",
        "            words.append(predicted_word)\n",
        "            if predicted_word == '[EOS]':\n",
        "                break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "model.eval()\n",
        "prompt = \"Los días de entresemana\"\n",
        "generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
