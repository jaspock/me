{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech\n",
    "\n",
    "pytorch backends:\n",
    "https://github.com/facebookresearch/demucs/issues/570\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m174.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 105M/2.26G [00:43<15:06, 2.56MB/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_data/speechcommands\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSPEECHCOMMANDS\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSPEECHCOMMANDS(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_specgram\u001b[39m(waveform, sample_rate, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/speechcommands.py:109\u001b[0m, in \u001b[0;36mSPEECHCOMMANDS.__init__\u001b[0;34m(self, root, url, folder_in_archive, download, subset)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(archive):\n\u001b[1;32m    108\u001b[0m             checksum \u001b[38;5;241m=\u001b[39m _CHECKSUMS\u001b[38;5;241m.\u001b[39mget(url, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 109\u001b[0m             \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m         _extract_tar(archive, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/hub.py:651\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mfile_size, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[1;32m    649\u001b[0m           unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m         buffer \u001b[38;5;241m=\u001b[39m \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    653\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/http/client.py:464\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 464\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"_data/speechcommands\", exist_ok=True)\n",
    "\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(\".\", subset=\"validation\", download=True)\n",
    "test_dataset = torchaudio.datasets.SPEECHCOMMANDS(\".\", subset=\"testing\", download=True)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\"):\n",
    "    waveform = waveform.numpy()\n",
    "    figure, ax = plt.subplots()\n",
    "    ax.specgram(waveform[0], Fs=sample_rate)\n",
    "    figure.suptitle(title)\n",
    "    figure.tight_layout()\n",
    "\n",
    "i = 0\n",
    "waveform, sample_rate, label = dataset[i]\n",
    "plot_specgram(waveform, sample_rate, title=f\"Sample {i}: {label}\")\n",
    "IPython.display.Audio(waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.49M/4.49M [00:03<00:00, 1.54MB/s]\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Batch: \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Función para visualizar espectrograma\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_spectrogram\u001b[39m(spectrogram):\n",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (spectrograms, targets, input_lengths, target_lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         output \u001b[39m=\u001b[39m model(spectrograms)  \u001b[39m# (batch, time, n_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:85\u001b[0m, in \u001b[0;36mYESNO.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m        labels\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m fileid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker[n]\n\u001b[0;32m---> 85\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_item(fileid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:65\u001b[0m, in \u001b[0;36mYESNO._load_item\u001b[0;34m(self, fileid, path)\u001b[0m\n\u001b[1;32m     63\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m fileid\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     64\u001b[0m file_audio \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, fileid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(file_audio)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m waveform, sample_rate, labels\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:203\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m    118\u001b[0m     uri: Union[BinaryIO, \u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    119\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     backend: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     backend \u001b[39m=\u001b[39m dispatcher(uri, \u001b[39mformat\u001b[39;49m, backend)\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:115\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mcan_decode(uri, \u001b[39mformat\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[39mreturn\u001b[39;00m backend\n\u001b[0;32m--> 115\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find appropriate backend to handle uri \u001b[39m\u001b[39m{\u001b[39;00muri\u001b[39m}\u001b[39;00m\u001b[39m and format \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.nn import Module, Conv2d, Linear, TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definición del modelo\n",
    "class SpeechRecognitionModel(Module):\n",
    "    def __init__(self, num_classes, input_size=128, num_heads=4, num_layers=3, hidden_size=256):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        # Capas convolucionales para reducir la dimensionalidad\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        # Transformer para el procesamiento secuencial\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "        # Capa lineal para el mapeo a la salida\n",
    "        self.fc = Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.permute(2, 0, 1)  # Cambiar a formato (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Cargar y procesar datos\n",
    "def data_processing(data):\n",
    "    spectrogram_transform = MelSpectrogram()\n",
    "    waveform, _, utterances, _, _, _ = zip(*data)\n",
    "    spectrograms = [spectrogram_transform(w).squeeze(0).transpose(0, 1) for w in waveform]\n",
    "    input_lengths = [len(s) for s in spectrograms]\n",
    "    target_lengths = [len(u) for u in utterances]\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    targets = [torch.tensor([ord(c) for c in u]) for u in utterances]\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    return spectrograms, targets, input_lengths, target_lengths\n",
    "\n",
    "# Configuración\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "num_classes = 29  # 26 letras + espacio, apóstrofe y caracter en blanco\n",
    "\n",
    "os.makedirs('_data', exist_ok=True)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "# Conjuntos de datos y cargadores\n",
    "train_dataset = LIBRISPEECH(\"./_data\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = LIBRISPEECH(\"./_data\", url=\"test-clean\", download=True)\n",
    "#train_dataset = torchaudio.datasets.YESNO(\"./_data\", download=True)\n",
    "#test_dataset = torchaudio.datasets.YESNO(\"./_data\", download=True)\n",
    "# play first example in the dataset in the notebook:\n",
    "waveform, sample_rate, utterance, _, _, _ = train_dataset[0]\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "#print its output labels:\n",
    "print(\"Targets:\", \"\".join([chr(int(i)+96) for i in utterance]))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_processing)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_processing)\n",
    "\n",
    "# Instanciar modelo y optimizador\n",
    "model = SpeechRecognitionModel(num_classes=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (spectrograms, targets, input_lengths, target_lengths) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Función para visualizar espectrograma\n",
    "def plot_spectrogram(spectrogram):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spectrogram.log2(), aspect='auto', origin='lower', \n",
    "               cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Reproducir un archivo de audio\n",
    "def play_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "# Prueba de visualización y reproducción de un archivo\n",
    "test_audio_path = \"./data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac\"\n",
    "waveform, sample_rate = torchaudio.load(test_audio_path)\n",
    "spectrogram = MelSpectrogram()(waveform)\n",
    "plot_spectrogram(spectrogram[0])\n",
    "play_audio(test_audio_path)\n",
    "\n",
    "# Evaluación del modelo con un solo archivo de audio\n",
    "def evaluate(model, audio_path):\n",
    "    model.eval()\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "    spectrogram = MelSpectrogram()(waveform).unsqueeze(0).transpose(2, 3)\n",
    "    with torch.no_grad():\n",
    "        output = model(spectrogram)\n",
    "        output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.transpose(0, 1).squeeze(0)\n",
    "        return ''.join([chr(o + 96) for o in output if o != 28])  # Convertir a texto\n",
    "\n",
    "# Uso de evaluate para probar un archivo\n",
    "print(evaluate(model, test_audio_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
