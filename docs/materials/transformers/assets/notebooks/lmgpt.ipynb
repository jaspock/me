{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Transformer-based language models\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio Pérez in 2024.\n",
        "\n",
        "This notebook presents \n",
        "\n",
        "**Exercise**: use spm to tokenize the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Nos ahorramos el padding en make_batch\n",
        "- Comentar sobre desconocidas y UNK en train y test\n",
        "- Hacer versión corta con MultiHeadAttention y LN de PyTorch\n",
        "- to-do: check that original multiheadattention behaves similar to the simpler one\n",
        "- mover transformer a notebook y poner un main que ejecuta uno muy pequeño inicializado aleatoriamente\n",
        "- wget github y %run 'note.ipynb' para ejecutarlo en celda de notebook\n",
        "- hacer una clase comun a encoder y decoder (solo cambia la mascara), TransformerModule, \n",
        "- EncoderOnlyTranformer, DecoderOnlyTransformer\n",
        "- poner set_seed en los otros notebooks y evaluarlo\n",
        "- añadir a NER el lr scheduler\n",
        "- Ejercicio: que funcione con minibatches en inferencia\n",
        "- Repasar el main del BERT para hacerlo como aquí\n",
        "- usar no_grad en todas la evaluaciones\n",
        "- avisar que no usaremos prefix tuning o como se llame (supongo que hay que usarla tb durante el training)\n",
        "- ejercicio: top-p sampling\n",
        "- sustituir !pip install por %pip en todos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def make_batch(corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    tokens = corpus.split()\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokens]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    batch_token_length = batch_size * max_len  # the total number of tokens in a batch\n",
        "    assert n_tokens >= batch_token_length, f'Short corpus ({n_tokens} tokens), must be at least {batch_length} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is the same as input, except shifted one token to the right\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 315\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "corpus = \"\"\"\n",
        "TEODORO:\t\n",
        "Fuese. ¿Quién pensó jamás\t\n",
        "de mujer tan noble y cuerda\t\n",
        "este arrojarse tan presto\t\n",
        "a dar su amor a entender?\t\n",
        "Pero también puede ser\t\n",
        "que yo me engañase en esto.\t\n",
        "Mas no me ha dicho jamás,\t\n",
        "ni a lo menos se me acuerda:\t\n",
        "«Pues ¿qué importa que se pierda,\t\n",
        "si se puede perder más?»\t\n",
        "Perder más... Bien puede ser\t\n",
        "por la mujer que decía...\t\n",
        "Mas todo es bachillería,\t\n",
        "y ella es la misma mujer.\t\n",
        "Aunque no, que la Condesa\t\n",
        "es tan discreta y tan varia\t\n",
        "que es la cosa más contraria\t\n",
        "de la ambición que profesa.\t\n",
        "Sírvenla príncipes hoy\t\n",
        "en Nápoles. ¿Qué no puedo\t\n",
        "ser su esclavo? Tengo miedo,\t\n",
        "que en grande peligro estoy.\t\n",
        "Ella sabe que a Marcela\t\n",
        "sirvo, pues aquí ha fundado\t\n",
        "el engaño y me ha burlado.\t\n",
        "Pero en vano se recela\t\n",
        "mi temor, porque jamás\t\n",
        "burlando salen colores.\t\n",
        "¿Y el decir con mil temores\t\n",
        "que se puede perder más?\t\n",
        "¿Qué rosa al llorar la Aurora\t\n",
        "hizo de las hojas ojos,\t\n",
        "abriendo los labios rojos\t\n",
        "con risa a ver cómo llora\t\n",
        "como ella los puso en mí,\t\n",
        "bañada en púrpura y grana,\t\n",
        "o qué pálida manzana\t\n",
        "se esmaltó de carmesí?\t\n",
        "Lo que veo y lo que escucho\t\n",
        "yo lo juzgo, o estoy loco,\t\n",
        "para ser de veras, poco,\t\n",
        "y para de burlas, mucho.\t\n",
        "Mas teneos, pensamiento,\t\n",
        "que os vais ya tras la grandeza,\t\n",
        "aunque si digo belleza\t\n",
        "bien sabéis vos que no miento,\t\n",
        "que es bellísima Diana\t\n",
        "y es discreción sin igual.\n",
        "\n",
        "MARCELA:\t\n",
        "¿Puedo hablarte?\n",
        "\n",
        "TEODORO:\t\n",
        "Ocasión tal\t\n",
        "mil imposibles allana,\t\n",
        "que por ti, Marcela mía,\t\n",
        "la muerte me es agradable.\n",
        "\n",
        "MARCELA:\t\n",
        "Como yo te vea y hable,\t\n",
        "dos mil vidas perdería.\t\n",
        "Estuve esperando el día\t\n",
        "como el pajarillo solo\t\n",
        "y, cuando vi que en el polo\t\n",
        "que Apolo más presto dora\t\n",
        "le despertaba la Aurora,\t\n",
        "dije: «Yo veré mi Apolo.»\t\n",
        "Grandes cosas han pasado,\t\n",
        "que no se quiso acostar\t\n",
        "la Condesa hasta dejar\t\n",
        "satisfecho su cuidado;\t\n",
        "amigas que han envidiado\t\n",
        "mi dicha con deslealtad\t\n",
        "le han contado la verdad,\t\n",
        "que entre quien sirve, aunque veas\t\n",
        "que hay amistad, no la creas,\t\n",
        "porque es fingida amistad.\t\n",
        "Todo lo sabe en efeto,\t\n",
        "que si es Diana la luna,\t\n",
        "siempre a quien ama importuna,\t\n",
        "salió y vio nuestro secreto;\t\n",
        "pero será, te prometo,\t\n",
        "para mayor bien, Teodoro,\t\n",
        "que del honesto decoro\t\n",
        "con que tratas de casarte\t\n",
        "le di parte, y dije aparte\t\n",
        "cuán tiernamente te adoro;\t\n",
        "tus prendas le encarecí,\t\n",
        "tu estilo, tu gentileza,\t\n",
        "y ella entonces su grandeza\t\n",
        "mostró tan piadosa en mí,\t\n",
        "que se alegró de que en ti\t\n",
        "hubiese los ojos puesto\t\n",
        "y de casarnos muy presto\t\n",
        "palabra también me dio,\t\n",
        "luego que de mí entendió\t\n",
        "que era tu amor tan honesto.\t\n",
        "Yo pensé que se enojara\t\n",
        "y la casa revolviera,\t\n",
        "que a los dos nos despidiera\t\n",
        "y a los demás castigara,\t\n",
        "mas su sangre ilustre y clara\t\n",
        "y aquel ingenio en efeto\t\n",
        "tan prudente y tan perfeto\t\n",
        "conoció lo que mereces.\t\n",
        "¡Oh, bien haya, amén mil veces,\t\n",
        "quien sirve a señor discreto!\n",
        "\n",
        "TEODORO:\t\n",
        "¿Que casarme prometió\t\n",
        "contigo?\n",
        "\n",
        "MARCELA:\t\n",
        "¿Pones duda\t\n",
        "que a su ilustre sangre acuda?\n",
        "\n",
        "TEODORO:\t\n",
        "Mi ignorancia me engañó\n",
        "\"\"\"\n",
        "\n",
        "word_list = list(set(corpus.split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1, '[EOS]': 2}\n",
        "special_tokens= len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "vocab_size = len(word_index)\n",
        "print(\"vocab size: %d\" % vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.12M\n",
            "Step     1, loss 5.754934\n",
            "Step   100, loss 5.313444\n",
            "Step   200, loss 3.775151\n",
            "Step   300, loss 1.498896\n",
            "Step   400, loss 0.850160\n",
            "Step   500, loss 0.571959\n",
            "Step   600, loss 0.430279\n",
            "Step   700, loss 0.237594\n",
            "Step   800, loss 0.189764\n",
            "Step   900, loss 0.180697\n",
            "Step  1000, loss 0.219462\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 4\n",
        "max_len = 32\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.001\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,  \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=training_steps, epochs=1, anneal_strategy='cos')\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "\n",
        "for inputs, outputs in make_batch(corpus, word_index, max_len, batch_size, device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if i % eval_steps == 0:\n",
        "        print(f'Step [{i}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEODORO: Cosas como estas son la cartilla, señora, de quien ama y quien desea. MARCELA: ¿Pones duda que a su ilustre sangre acuda? TEODORO: Mi ignorancia me engañó TEODORO: Fuese. ¿Quién pensó\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_text(model, prompt, word_index, index_word, max_len, device):\n",
        "    words = prompt.split()\n",
        "    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n",
        "    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(input_ids)):\n",
        "            output = model(input)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n",
        "            predicted_word = index_word[predicted_id]\n",
        "            words.append(predicted_word)\n",
        "            if predicted_word == '[EOS]':\n",
        "                break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "model.eval()\n",
        "prompt = \"\"\"TEODORO: \n",
        "Cosas como estas\n",
        "son la cartilla, señora,\t\n",
        "de quien ama y quien desea.\n",
        "\n",
        "MARCELA:\"\"\"\n",
        "generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
