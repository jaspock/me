{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: torchaudio in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: jiwer in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: sox in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (1.4.1)\n",
      "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jiwer) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sox) (1.26.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchaudio jiwer sox\n",
    "\n",
    "# ubuntu: sudo apt install libsox-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "the feature number of src and tgt must be equal to d_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Test function\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(model, test_loader):\n",
      "Cell \u001b[0;32mIn[20], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m spectrograms, labels \u001b[38;5;241m=\u001b[39m spectrograms\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 77\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrograms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m, in \u001b[0;36mASRModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Rearrange dimensions for transformer\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/transformer.py:202\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    205\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[1;32m    206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    207\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    208\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    209\u001b[0m                       tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal, memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: the feature number of src and tgt must be equal to d_model"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(\"_data/speechcommands\", exist_ok=True)\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(\"_data/speechcommands\", url=\"https://github.com/jaspock/me/raw/main/docs/materials/transformers/assets/datasets/filtered_datasets/speech_commands_v0.02.tar.gz\", subset=\"validation\", download=True)\n",
    "test_dataset = torchaudio.datasets.SPEECHCOMMANDS(\"_data/speechcommands\", url=\"https://github.com/jaspock/me/raw/main/docs/materials/transformers/assets/datasets/filtered_datasets/speech_commands_v0.02.tar.gz\", subset=\"testing\", download=True)\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure data directory exists\n",
    "os.makedirs(\"_data/speechcommands\", exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchaudio.datasets.SPEECHCOMMANDS(\"_data/speechcommands\", subset=\"validation\", download=True)\n",
    "test_dataset = torchaudio.datasets.SPEECHCOMMANDS(\"_data/speechcommands\", subset=\"testing\", download=True)\n",
    "\n",
    "# Model Definition\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, num_classes=29, input_size=128, embd_size=64):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, input_size, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(input_size, input_size, kernel_size=3, stride=2, padding=1)\n",
    "        self.transformer = Transformer(d_model=embd_size, nhead=2, num_encoder_layers=3, \n",
    "                                       num_decoder_layers=3, dim_feedforward=2*embd_size, dropout=0.1)\n",
    "        self.fc = nn.Linear(embd_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.transpose(0, 2).transpose(1, 2)  # Rearrange dimensions for transformer\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.fc(x)\n",
    "        return x.transpose(0, 1)  # Adjust dimensions for CrossEntropyLoss\n",
    "\n",
    "n_mels = 128\n",
    "\n",
    "# Data processing function\n",
    "def data_processing(batch):\n",
    "    spectrogram_transform = MelSpectrogram(n_mels=n_mels, sample_rate=16000)\n",
    "    tensors, labels = [], []  # tensors is a Python list of tensors\n",
    "    for waveform, _, utterance, _, _ in batch:\n",
    "        # waveform 1xT', T'=samples\n",
    "        spec = spectrogram_transform(waveform)  # (1, C, T), C=n_mels, T=frames\n",
    "        spec = spec.squeeze(0)  # (C, T)\n",
    "        spec = spec.transpose(0, 1)  # (T, C)\n",
    "        tensors.append(spec)\n",
    "        labels.append(torch.tensor([ord(c) - 96 for c in utterance]))  # Label processing\n",
    "    tensors = nn.utils.rnn.pad_sequence(tensors, batch_first=True)  \n",
    "    tensors = tensors.transpose(1, 2)  # (B, C, T)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return tensors, labels\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_processing)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=data_processing)\n",
    "\n",
    "# Model initialization\n",
    "model = ASRModel(input_size=n_mels).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrograms)\n",
    "            print(output.shape)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "# Training the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Test function\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels in test_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = torch.max(outputs.data, 2)\n",
    "            total += labels.size(0) * labels.size(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# Testing the model\n",
    "test(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.49M/4.49M [00:03<00:00, 1.54MB/s]\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Batch: \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Función para visualizar espectrograma\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_spectrogram\u001b[39m(spectrogram):\n",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (spectrograms, targets, input_lengths, target_lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         output \u001b[39m=\u001b[39m model(spectrograms)  \u001b[39m# (batch, time, n_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:85\u001b[0m, in \u001b[0;36mYESNO.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m        labels\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m fileid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker[n]\n\u001b[0;32m---> 85\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_item(fileid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:65\u001b[0m, in \u001b[0;36mYESNO._load_item\u001b[0;34m(self, fileid, path)\u001b[0m\n\u001b[1;32m     63\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m fileid\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     64\u001b[0m file_audio \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, fileid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(file_audio)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m waveform, sample_rate, labels\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:203\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m    118\u001b[0m     uri: Union[BinaryIO, \u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    119\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     backend: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     backend \u001b[39m=\u001b[39m dispatcher(uri, \u001b[39mformat\u001b[39;49m, backend)\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:115\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mcan_decode(uri, \u001b[39mformat\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[39mreturn\u001b[39;00m backend\n\u001b[0;32m--> 115\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find appropriate backend to handle uri \u001b[39m\u001b[39m{\u001b[39;00muri\u001b[39m}\u001b[39;00m\u001b[39m and format \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.nn import Module, Conv2d, Linear, TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definición del modelo\n",
    "class SpeechRecognitionModel(Module):\n",
    "    def __init__(self, num_classes, input_size=128, num_heads=4, num_layers=3, hidden_size=256):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        # Capas convolucionales para reducir la dimensionalidad\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        # Transformer para el procesamiento secuencial\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "        # Capa lineal para el mapeo a la salida\n",
    "        self.fc = Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.permute(2, 0, 1)  # Cambiar a formato (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Cargar y procesar datos\n",
    "def data_processing(data):\n",
    "    spectrogram_transform = MelSpectrogram()\n",
    "    waveform, _, utterances, _, _, _ = zip(*data)\n",
    "    spectrograms = [spectrogram_transform(w).squeeze(0).transpose(0, 1) for w in waveform]\n",
    "    input_lengths = [len(s) for s in spectrograms]\n",
    "    target_lengths = [len(u) for u in utterances]\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    targets = [torch.tensor([ord(c) for c in u]) for u in utterances]\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    return spectrograms, targets, input_lengths, target_lengths\n",
    "\n",
    "# Configuración\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "num_classes = 29  # 26 letras + espacio, apóstrofe y caracter en blanco\n",
    "\n",
    "os.makedirs('_data', exist_ok=True)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "# Conjuntos de datos y cargadores\n",
    "train_dataset = LIBRISPEECH(\"./_data\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = LIBRISPEECH(\"./_data\", url=\"test-clean\", download=True)\n",
    "#train_dataset = torchaudio.datasets.YESNO(\"./_data\", download=True)\n",
    "#test_dataset = torchaudio.datasets.YESNO(\"./_data\", download=True)\n",
    "# play first example in the dataset in the notebook:\n",
    "waveform, sample_rate, utterance, _, _, _ = train_dataset[0]\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "#print its output labels:\n",
    "print(\"Targets:\", \"\".join([chr(int(i)+96) for i in utterance]))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_processing)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_processing)\n",
    "\n",
    "# Instanciar modelo y optimizador\n",
    "model = SpeechRecognitionModel(num_classes=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (spectrograms, targets, input_lengths, target_lengths) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Función para visualizar espectrograma\n",
    "def plot_spectrogram(spectrogram):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spectrogram.log2(), aspect='auto', origin='lower', \n",
    "               cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Reproducir un archivo de audio\n",
    "def play_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "# Prueba de visualización y reproducción de un archivo\n",
    "test_audio_path = \"./data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac\"\n",
    "waveform, sample_rate = torchaudio.load(test_audio_path)\n",
    "spectrogram = MelSpectrogram()(waveform)\n",
    "plot_spectrogram(spectrogram[0])\n",
    "play_audio(test_audio_path)\n",
    "\n",
    "# Evaluación del modelo con un solo archivo de audio\n",
    "def evaluate(model, audio_path):\n",
    "    model.eval()\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "    spectrogram = MelSpectrogram()(waveform).unsqueeze(0).transpose(2, 3)\n",
    "    with torch.no_grad():\n",
    "        output = model(spectrogram)\n",
    "        output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.transpose(0, 1).squeeze(0)\n",
    "        return ''.join([chr(o + 96) for o in output if o != 28])  # Convertir a texto\n",
    "\n",
    "# Uso de evaluate para probar un archivo\n",
    "print(evaluate(model, test_audio_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
