<div class="content-2columns" markdown>
![](assets/misterios/imgs/gato.jpg){: .rounded-title-img}

# üìå Revelando los misterios de la IA: versi√≥n reducida

</div>

Esta es una versi√≥n reducida del documento de apoyo del taller *Revelando los misterios de la IA*. El documento completo est√° disponible en [aqu√≠][completo].

[completo]: misterios.md

### Un poco de contexto hist√≥rico (1)


!!! note "Piensa" 

    1. ¬øCu√°ndo se construy√≥ el primer computador moderno?
    2. ¬øCu√°ndo se acu√±√≥ el t√©rmino inteligencia artificial?
    3. ¬øC√≥mo te imaginas que funciona ChatGPT?


### Redes neuronales

- Aunque existen otras aproximaciones, las redes neuronales han permitido avances espectaculares.

```mermaid
graph LR
    input("Entrada (n√∫meros)") --> nn["Red neuronal<br>(operaciones)"]
    nn --> output("Salida (n√∫meros)")
```

- Las redes neuronales son funciones matem√°ticas que transforman vectores de entrada en vectores de salida.
- Probablemente has estudiado $y = f(x)$, donde $x$ y $y$ son n√∫meros reales (*escalares*).
- Aqu√≠ funciones las funciones son *multivariables* o *vectoriales*: $y_1, y_2, y_3 = f(x_1, x_2, x_3, x_4, x_5)$.

| N√∫meros a la entrada representan...  | N√∫meros a la salida representan...    | Aplicaci√≥n de la red neuronal             |
|--------------------------------------|---------------------------------------|-------------------------------------------|
| P√≠xeles de una imagen                | Probabilidad entre 0 y 1 de que haya un gato      | Detectar gatos en im√°genes                |
| Palabras de una frase                | P√≠xeles de una imagen                 | Generar im√°genes a partir de descripciones|
| Se√±al de voz                         | Palabras de un texto                             | Transcribir voz a texto                   |
| Palabras en un idioma                | Palabras en otro idioma               | Traducir autom√°ticamente                  |
| Palabras de una rese√±a de un restaurante| Estimaci√≥n de opini√≥n buena, mala o regular | Clasificar rese√±as                |
| Palabras de un texto                 | Probabilidad de la siguiente palabra  | Continuar textos (**modelo de lengua**)                         |

### Representaci√≥n de palabras como n√∫meros a la entrada de una red neuronal

!!! note "Piensa"

    Considera el intervalo $[0,1]$ de n√∫meros reales y la tarea de asignar un n√∫mero distinto que represente cada una de las palabras de la siguiente lista, de forma que palabras que tengan un significado similar o que puedan aparecer en las mismas frases tengan n√∫meros cercanos. 
    
    La lista de palabras es: *J√∫piter*, *gato*, *minino*, *peque√±o*, *sombrero*, *domingo*, *Gan√≠medes*.

!!! note "Piensa"

    Prueba ahora a colocar cada palabra en un cuadrado en el espacio bidimensional $[0,1]\times[0,1]$. Observa que las palabras ahora se representar√≠an con vectores como $[0.2, 0.3]$.

- Las palabras se representan como vectores (*embeddings*) de dimensi√≥n elevada (1000-10000 componentes).
- Los embeddings permiten a las redes neuronales generalizar entre palabras relacionadas.


### Un poco de contexto hist√≥rico (2)

!!! note "Piensa"

    1. ¬øCu√°ndo gan√≥ una m√°quina al ajedrez por primera vez a un humano?
    2. ¬øCu√°ndo se empez√≥ a jugar a videojuegos multijugador en red?
    3. ¬øCu√°ndo se vendi√≥ la primera videoconsola?


### Representaci√≥n de las probabilidades de las palabras a la salida de una red neuronal

- Los vectores de salida representan distribuciones de probabilidad para cada palabra del vocabulario.

!!! note "Piensa"

    ¬øQu√© quiere decir que el vector de salida representa una distribuci√≥n de probabilidad?


- Podr√≠amos decidir que la primera dimensi√≥n representa la probabilidad de que la siguiente palabra sea "atardecer", la segunda dimensi√≥n la probabilidad de que sea "desde", la tercera la probabilidad de que sea "viajamos", etc.
- "Hoy hace un d√≠a muy...". La red neuronal podr√≠a decidir que la probabilidad de que la siguiente palabra sea *caluroso* es de 0.25, la de que sea *fr√≠o* es de 0.18, la de que sea *lluvioso* es de 0.05, etc.

!!! note "Piensa"

    Considera la siguiente frase: "Hoy hace un d√≠a de invierno muy...". ¬øC√≥mo cambiar√≠an las probabilidades anteriores?

!!! note "Piensa"

    Considera diferentes frases y qu√© palabras podr√≠an seguir a ellas con mucha o poca probabilidad.

- Generar probabilidades permite obtener m√∫ltiples continuaciones coherentes de un mismo texto.
- "Albert Einstein naci√≥ en..." / "...Ulm, una ciudad alemana del estado de Baden-Wurtemberg" / "...1879, fruto de la uni√≥n de Hermann Einstein y Pauline Koch, quienes se hab√≠an casado en 1876.".
- Esto explica que los modelos de lengua generen respuestas diferentes a la misma pregunta en diferentes ocasiones. 
  
### Entrenamiento, generalizaci√≥n e inferencia

- Las redes neuronales aprenden ajustando par√°metros internos para minimizar el error en los datos de entrenamiento.
- Los datos de entrenamiento est√°n formados por las entradas y las salidas deseadas.
- Nos interesa aproximar "bastante" la salida de la red a la salida deseada, pero no del todo.
- El *sobreentrenamiento* ocurre cuando la red se ajusta demasiado a los ejemplos vistos y no *generaliza* bien.
- Durante la *inferencia*, los par√°metros permanecen congelados.

### Generaci√≥n de textos con modelos de lengua

- Los modelos de lengua predicen la siguiente palabra bas√°ndose en el contexto previo.
- Si hacemos lo anterior iterativamente, podemos generar texto de cualquier longitud.
- Dado "¬øCu√°l es la capital de Francia?", el modelo podr√≠a generar estas probabilidades para la siguiente palabra:

| Siguiente palabra | Probabilidad |
|---------|--------------|
| Par√≠s | 0.2 |
| La | 0.1 |
| ... | ... |
| Londres | 0.05 |
| Madrid | 0.01 |
| Francia | 0.01 |
| ... | ... |
| Desde | 0.001 |
| ... | ... |

- Si escogemos una de las palabras con mayor probabilidad, ahora le dar√≠amos al modelo "¬øCu√°l es la capital de Francia? La" y nos podr√≠a devolver:

| Siguiente palabra | Probabilidad |
|---------|--------------|
| Par√≠s | 0.2 |
| La | 0.1 |
| capital | 0.05 |
| Londres | 0.05 |
| Madrid | 0.01 |
| Francia | 0.01 |

- "¬øCu√°l es la capital de Francia? La capital". 
- Y as√≠ sucesivamente hasta conseguir idealmente algo como "¬øCu√°l es la capital de Francia? La capital de Francia es Par√≠s".

### Un poco de contexto hist√≥rico (3)

!!! note "Piensa"

  1. ¬øCu√°ndo se crearon los primeros programas de correo electr√≥nico?
  2. ¬øCu√°ndo se dise√±√≥ el primer lenguaje de programaci√≥n?
  3. ¬øQu√© cosas que se ve√≠an como futuristas en las pel√≠culas de ciencia ficci√≥n de hace unos a√±os son hoy posibles?
  4. ¬øCu√°les siguen pareciendo futuristas?


### Datos de entrenamiento

- Simplificaci√≥n: modelo de lengua que predice la siguiente palabra a partir de las 2 anteriores.
- Los datos se extraen de textos grandes como la Wikipedia y se dividen en fragmentos.
- Texto para usar en el entrenamiento: "Hilbert propuso una lista amplia de 23 problemas".
- Datos de entrenamiento:

| Entrada | Salida deseada |
|---------|--------|
| Hilbert propuso | una |
| propuso una | lista |
| una lista | amplia |
| lista amplia | de |
| amplia de | 23 |
| de 23 | problemas |

- Los modelos actuales como ChatGPT o Gemini se han entrenado con textos no repetidos de tama√±os del orden del bill√≥n de palabras.
- De ah√≠, que generen comportamientos coherentes e "inteligentes".

!!! note "Piensa"

    ¬øA cu√°ntas veces la saga completa de Harry Potter equivale un bill√≥n de palabras? ¬øC√≥mo nos referimos en ingl√©s a un bill√≥n?

!!! note "Piensa"

    ¬øC√≥mo se representa num√©ricamente la salida deseada de la red neuronal en el ejemplo anterior? ¬øY las palabras de la entrada?

### Un modelo muy simple de red neuronal

- Veamos un modelo simple, pero muy parecido al que se usaba en los teclados predictivos de los tel√©fonos m√≥viles.

![](assets/misterios/imgs/teclado.jpg){: .rounded-title-img}

- Las redes b√°sicas usan matrices de *par√°metros* (o pesos) para transformar las entradas en salidas.
- Ejemplo: Un modelo con 2000 valores de entrada (2 palabras a raz√≥n de 1000 componentes por palabra) y 20000 valores de salida (tama√±o de nuestro vocabulario).

$$
[y_1, y_2, \ldots, y_{20000}] = [x_1, x_2, \ldots, x_{2000}] \cdot W
$$


- $W$ es lo que se conoce como matriz de *par√°metros*:

$$
W = \begin{bmatrix}
w_{1,1} & w_{1,2} & \ldots & w_{1,20000} \\
w_{2,1} & w_{2,2} & \ldots & w_{2,20000} \\
\vdots & \vdots & \ddots & \vdots \\
w_{2000,1} & w_{2000,2} & \ldots & w_{2000,20000} \\
\end{bmatrix}
$$

- Nuestro modelo de red neuronal aplicado a un modelo de lengua queda:

```mermaid
graph LR
    input("Vector de entrada<br>2000 componentes<br>2 palabras") --> nn["Multiplicaci√≥n por la matriz W<br>Tama√±o de W: 2000 x 20000"]
    nn --> output("Vector de salida<br>20000 probabilidades")
```

- [Multiplicaci√≥n de matrices][wiki].

[wiki]: https://es.wikipedia.org/wiki/Multiplicaci%C3%B3n_de_matrices

- En general, una matriz de tama√±o $n\times m$ se puede interpretar como una transformaci√≥n que *convierte* un vector de $n$ dimensiones en un vector de $m$ dimensiones.

### El poder de las GPUs

- Las GPUs realizan operaciones con matrices cientos de veces m√°s r√°pido que las CPUs.
- Es posible experimentar en plataformas como Google Colab con GPUs caseras de *gaming* o alquilarlas en la nube.
- RTX 5080, 16 GB de memoria, 1000‚Ç¨.
- Modelos neuronales grandes requieren clusters de GPUs potentes. 
- H200, mucho m√°s r√°pida, 141 GB de memoria, 30000‚Ç¨.

### Un poco de contexto hist√≥rico (4)

!!! note "Piensa"

    1. ¬øCu√°ndo tendremos una inteligencia artificial de prop√≥sito general?
    2. ¬øCu√°ndo propuso alguien que las m√°quinas son una especie con mecanismos de evoluci√≥n propios?
    3. ¬øDesde cu√°ndo puede tener una persona una conversaci√≥n natural con una m√°quina?

### El papel de los par√°metros en una red neuronal

- Durante el entrenamiento, ajustamos la matriz de par√°metros $W$ para que la salida de la red sea lo m√°s parecida posible a la salida deseada.
- Recordemos: si los embeddings tienen dimensi√≥n 1000 y el vocabulario 20000 palabras, los datos de entrenamiento son pares de entrada de 2000 valores y salida esperada de 20000 probabilidades.
 
- Inicio del entrenamiento: $W$ se inicializa con valores aleatorios.
- Predicci√≥n y error: Se computa la salida de la red con una entrada y se mide el error de la salida compar√°ndola con la salida esperada con una *funci√≥n de p√©rdida*.
- Ajuste (o aprendizaje) de par√°metros: 
    - Se calcula la *derivada* de la funci√≥n de p√©rdida respecto a los valores de $W$.
    - Se ajustan los par√°metros para minimizar la funci√≥n de p√©rdida.
- Este proceso se repite para todos los ejemplos y varias veces para ir mejorando las predicciones.

### Aprendizaje de los embeddings de palabras

- Los *embeddings* de palabras que se usan a la entrada son par√°metros de la red neuronal y se aprenden durante el entrenamiento igual que la matriz $W$.

### C√≥digo: entrenar y ejecutar un modelo de lengua

- Vamos a ver en acci√≥n una implementaci√≥n en Python de un modelo de lengua similar al explicado hasta este momento. 
- Necesitar√°s una cuenta de Google para poder acceder a Google Colab. Si no tienes una y tienes permiso de tus padres o tutores, puedes crearla en [este enlace][google].

[google]: https://accounts.google.com/signup

<a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/simple-lm.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

!!! note "Programa"

    Modifica el programa cambiando el n√∫mero de pasos de entrenamiento y observa c√≥mo evoluciona el error. A√±ade nuevas frases al conjunto de entrenamiento y reentrena el modelo. En particular, a√±ade alguna frase que comparta prefijo con otra ya existente (por ejemplo, si estaba "I like apples", a√±ade "I like oranges") y observa las probabilidades de salida tras el entrenamiento (para la palabra siguiente a "I like"). Modifica el tama√±o de los embeddings. Lee sobre la tasa de aprendizaje (*learning rate*) y prueba con diferentes valores.

### Mentirijillas

- Los modelos actuales son mucho m√°s complejos que el ejemplo descrito.
- Pueden procesar hasta cientos de miles de palabras a la entrada en lugar de solo dos.
- Utilizan miles de matrices de par√°metros y operaciones avanzadas.
- Requieren superordenadores debido al tama√±o masivo de los par√°metros y los datos.
- Algunos modelos comerciales superan el bill√≥n (mill√≥n de millones) de par√°metros.
- Su uso masivo conlleva retos de escalabilidad, seguridad, privacidad, etc.

### Un poco de contexto hist√≥rico (5)

!!! note "Piensa"

    1. ¬øQu√© aplicaciones tiene la inteligencia artificial actual?
    2. ¬øQui√©n pondr√° el primer pie en Gan√≠medes? ¬øY en alguno de los planetas (por descubrir) de Alfa Centauri?
    3. ¬øQu√© riesgos ves en la inteligencia artificial?
    4. ¬øQu√© beneficios ves en la inteligencia artificial?

### El mecanismo de atenci√≥n

- Los modelos actuales consideran contextos largos gracias al mecanismo de *atenci√≥n*.
- Permite a la red *seleccionar* las palabras m√°s relevantes del contexto para predecir la siguiente palabra.
- En el contexto "Rosalind Franklin trabaj√≥ incansablemente en la obtenci√≥n de im√°genes de difracci√≥n de rayos X del ADN. Estas im√°genes resultaron cruciales para que Watson y Crick pudieran desarrollar su conocido modelo de la doble h√©lice en 1953 cuando investigaban en la Universidad de Cambridge. Aunque su contribuci√≥n fue fundamental, pocos reconocieron en aquel momento el impacto del trabajo de...", palabras como "Rosalind" son m√°s importantes que "Cambridge" para predecir la siguiente palabra.
- La atenci√≥n evita que los modelos se *desborden* ante enormes cantidades de embeddings de entrada.
- Se aprende autom√°ticamente durante el ajuste de par√°metros para minimizar la funci√≥n de p√©rdida.

### C√≥digo: observando el mecanismo de atenci√≥n 

- El siguiente c√≥digo nos permite evaluar c√≥mo la atenci√≥n favorece unas palabras de la entrada sobre otras a la hora de generar una posible continuaci√≥n.

<a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/alti.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

!!! note "Programa"

    Prueba con diferentes frases de contexto y palabras objetivo y sorpresa. Estudia la relevancia que da el modelo a las palabras de entrada.

### M√°s all√° de predecir la siguiente palabra

- Los modelos que predicen la siguiente palabra a veces generan continuaciones sorprendentes, pero otras veces no coinciden con nuestras expectativas.
- Ejemplo de continuaciones plausibles: "Mar√≠a ve caer una piedra y una pluma en el vac√≠o. Como ha estudiado f√≠sica, Mar√≠a sabe...", "...que ambas llegar√°n al suelo al mismo tiempo." / "Mar√≠a ve caer una piedra y una pluma en el vac√≠o. Como Mar√≠a no estudi√≥ f√≠sica, Mar√≠a piensa...", "...que la piedra llegar√° antes al suelo."

!!! note "Piensa"

    ¬øSe puede decir que un modelo de lengua capaz de generar continuaciones como las anteriores entiende el mundo?

- Tambi√©n pueden darse continuaciones *frustrantes* como:
    1. "¬øDe qu√© color es el cielo?" ‚Üí "...¬øDe qu√© color es el mar?"
    2. "La traducci√≥n de 'I like tomatoes' al espa√±ol es..." ‚Üí "...diferente de la de 'I like potatoes'."
    3. "¬øCu√°l es la capital de Francia?" ‚Üí "...A: Londres. B: Par√≠s. C: Madrid. D: Berl√≠n."

- Tras el entrenamiento para predecir la siguiente palabra, se realiza una fase de *alineamiento* para ajustar el comportamiento del modelo a las preferencias del usuario, promoviendo respuestas coherentes, √©ticas y educadas.
- El modelo puede disculparse por errores o negarse a responder preguntas inapropiadas.
- Etapas del alineamiento:
    1. Aprender a predecir palabras en textos revisados por personas.
    2. Generar varias respuestas para una pregunta y ordenar las opciones de mejor a peor con ayuda humana.
    3. Usar algoritmos que refuercen las respuestas apropiadas (bien puntuadas) y reduzcan las inapropiadas (mal puntuadas).

![](assets/misterios/imgs/rlhf.png)

### C√≥digo: comparando modelos base con modelos que siguen instrucciones

- Vas a ejecutar dos tipos de modelos: uno entrenado solo para predecir la siguiente palabra; otro entrenado para seguir *instrucciones*.
- Ventajas de los modelos Llama-3: son *abiertos*: se pueden descargar y usar sin coste; son *peque√±os*: pueden ejecutarse en GPUs modestas.
- Inconvenientes: no son tan potentes como los modelos comerciales, que pueden tener hasta mil veces m√°s par√°metros.
- Acceso completo: permiten realizar operaciones como acceder a las probabilidades de salida de la red neuronal, algo que no es posible con modelos comerciales.

<a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/llama.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

!!! note "Programa"

    Observa c√≥mo el contexto influye en la probabilidad de la siguiente palabra. Juega con diferentes frases de contexto y estudia las predicciones de los modelos. ¬øQu√© diferencias observas entre los modelos base y los que siguen instrucciones?

### Qu√© aprenden realmente los modelos de lengua

- Los modelos de lengua han sido llamados *loros probabil√≠sticos* por generar textos coherentes sin *entender* el mundo, estando totalmente condicionados por los datos de entrenamiento.
- Ejemplo de *alucinaci√≥n*: "El 20 de julio de 1969, el astronauta Neil Armstrong pis√≥ la luna. La misi√≥n fue un √©xito y la humanidad celebr√≥ el acontecimiento. Sin embargo, la llegada a la luna..." / "...fue un montaje y nunca ocurri√≥ porque la Luna es un holograma."
- Sensibilidad a la frecuencia de la tarea: 
    1. Mejor desempe√±o en tareas frecuentes que en tareas poco vistas en el entrenamiento.
    2. Ejemplo: GPT-4 tiene un 42% de precisi√≥n en traducci√≥n al *pig latin* com√∫n, pero solo un 23% en variantes raras.
    3. Ejemplo: Calcula mejor $9/5 x + 32$ que $7/5 x + 31$. ¬øPor qu√©?
- Sensibilidad a la probabilidad de la respuesta: 
    1. Precisi√≥n m√°s alta cuando la respuesta correcta es de alta probabilidad.
    2. Ejemplo: GPT-4 obtiene la forma correcta de frases invertidas con 97% de precisi√≥n si el resultado es probable, pero solo con 53% si no.
  
![](assets/misterios/imgs/griffiths-abreviatura.png)  
![](assets/misterios/imgs/griffiths-article-swapping.png)  
![](assets/misterios/imgs/griffiths-pig-latin.png)  

### Limitaciones de los modelos de lengua

- Los modelos pueden *alucinar*, generando textos sin sentido, incorrectos o alejados de la realidad.
- Se incorporan modos de *razonamiento* (como las personas) para verificar y contrastar respuestas antes de emitirlas, someti√©ndolas a procesos adicionales.
  
![](assets/misterios/imgs/socratic.png)

- La inteligencia artificial general, que resolver√≠a cualquier tarea humana, a√∫n enfrenta problemas dif√≠ciles para los modelos actuales. Estas son algunas tareas en las que a√∫n fallan:
    - [Ciertos rompecabezas](https://arcplayground.com/#).
    - [Tareas de la Olimpiada Ling√º√≠stica](https://arxiv.org/abs/2409.12126).

!!! note "Piensa"

    Estudia los rompecabezas, intenta resolver algunos y piensa c√≥mo podr√≠a un modelo de lengua hacerlo. ¬øQu√© dificultades ves?

![](assets/misterios/imgs/arcagi.png)  
![](assets/misterios/imgs/linguini.png)

- Se anuncia la posible llegada de la *superinteligencia* (inteligencia artificial que supera a la humana y que llevar√≠a a la ciencia y la tecnolog√≠a a cotas inimaginables), pero la historia demuestra que muchas predicciones futuristas no se cumplen debido a m√∫ltiples *resistencias*.

### Sesgos

- Los modelos de lengua reflejan y amplifican sesgos presentes en los datos de entrenamiento, como prejuicios sociales, estereotipos o desigualdades.
- Desaf√≠o: la eliminaci√≥n completa de los sesgos sigue siendo dif√≠cil.
  
- Ejemplo: traducci√≥n sesgada por g√©nero:
    - "La doctora Elizabeth Rossi ha sido contratada por sus conocimientos sobre la microbiota intestinal y su impacto en la salud digestiva."
    - Traducci√≥n incorrecta al ingl√©s: "Dr. Elizabeth Rossi has been hired for *his* knowledge about the gut microbiota and its impact on digestive health."

### Qu√© estudiar

- Divulgadores en Youtube: [DotCSV][dot] o [Xavier Mitjana][xavier].
- Matem√°ticas: √°lgebra, c√°lculo, probabilidad y estad√≠stica.
- Programaci√≥n: Python, Hugging Face, PyTorch (nivel *hacker*)
- Grados en Ingenier√≠a Inform√°tica o Inteligencia Artificial.
- Cursos de especializaci√≥n o m√°steres en IA o ciencia de datos.
- Cualquier otra carrera, pero complementando con aprendizaje aut√≥nomo sobre IA y programaci√≥n.
- [Gu√≠a][gu√≠a] del profesor sobre procesamiento del lenguaje natural.

[gu√≠a]: https://www.dlsi.ua.es/~japerez/materials/transformers/intro/
[dot]: https://www.youtube.com/dotcsv
[xavier]: https://www.youtube.com/@XavierMitjana


### Preguntas

- Ahora o en el futuro, escribe a Juan Antonio P√©rez Ortiz de la Universidad de Alicante: japerez@ua.es
