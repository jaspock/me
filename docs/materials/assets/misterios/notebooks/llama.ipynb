{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yK6TSC8OfG"
      },
      "source": [
        "# Demo de dos modelos de lengua\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/llama.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> <a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Cuaderno escrito por Juan Antonio P칠rez en 2025. Este cuaderno permite evaluar las continuaciones y las probabilidades de la siguiente palabra seg칰n dos modelos de lengua, uno entrenado 칰nicamente para predecir el siguiente token y otro entrenado para seguir instrucciones.\n",
        "\n",
        "El entorno de ejecuci칩n de este cuaderno ha de tener una GPU. En Google Colab, la puedes conseguir desde el men칰 *Entorno de ejecuci칩n* / *Cambiar tipo de entorno de ejecuci칩n*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMpSvW7yBlBm"
      },
      "source": [
        "## Inicializaci칩n de los modelos\n",
        "\n",
        "El siguiente c칩digo inicializa el modelo base y el modelo que sigue instrucciones a partir de los modelos abiertos de tama침o 1B (1 millardo de par치metros) de la familia [Llama-3.2](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) liberados por [Meta](https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/). Para que funcione, has de tener una clave secreta de nombre `HF_TOKEN` con un valor que puedes obtener si te creas una cuenta en [Hugging Face](https://huggingface.co/). Pregunta a tu profesor antes. Esta clave se a침ade desde la secci칩n con el icono de la llave a la izquierda de este cuaderno, si lo has abierto en Google Colab.\n",
        "\n",
        "La descarga de los modelos puede llevar unos minutos, especialmente la primera vez que se ejecute el cuaderno en un nuevo entorno de ejecuci칩n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egT2Bnqt2182"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Base model setup\n",
        "# base_model_id = \"meta-llama/Llama-3.2-3B\"\n",
        "base_model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "base_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                                  torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "# Instruction-tuned model setup\n",
        "# instruction_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "instruction_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "instruction_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=instruction_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "instruction_tokenizer = AutoTokenizer.from_pretrained(instruction_model_id)\n",
        "instruction_model = AutoModelForCausalLM.from_pretrained(instruction_model_id,\n",
        "                                                         torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jQDVzlm0VPJ"
      },
      "source": [
        "## Obtenci칩n de las probabilidades de salida\n",
        "\n",
        "La funci칩n `get_word_probs` devuelve un diccionario de Python con las probabilidades del modelo para las palabras de la lista `candidates` cuando se ha procesado el contexto `text` con el modelo `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO_eZRt-ozG-"
      },
      "outputs": [],
      "source": [
        "def get_word_probs(model, tokenizer, text, candidates):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[0, -1]  # last token logits\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    candidate_probs = {}  # initial dictionary\n",
        "    for word in candidates:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(word)\n",
        "        if token_id is not None:\n",
        "            candidate_probs[word] = probs[token_id].item()\n",
        "        else:\n",
        "            candidate_probs[word] = 0.0  # word not in embedding table\n",
        "\n",
        "    return candidate_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaEcUBhQ9_qs"
      },
      "source": [
        "## Probando, probando...\n",
        "\n",
        "En primer lugar, probaremos que los modelos funcionan pidi칠ndoles que contin칰en un texto.\n",
        "\n",
        "Si el valor de `do_sample` est치 a `True`, el sistema genera salidas diferentes cada vez. La diversidad en ese caso se puede ajustar jugando con los argumentos siguientes:\n",
        "\n",
        "- `top_k`: limita el n칰mero de palabras (tokens) m치s probables a considerar; por ejemplo, `top_k=50`\n",
        "- `top_p`: elige palabras hasta alcanzar una probabilidad acumulada espec칤fica; por ejemplo, `top_p=0.95`\n",
        "- `temperature`: controla la aleatoriedad: valores bajos entre 0 y 1 son m치s conservadores, altos entre 1 y 2 m치s creativos, pero aumentando el riesgo de incoherencias; es habitual hacer `temperature=0.7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-JbDD36-AEW"
      },
      "outputs": [],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "\n",
        "inputs_normal = base_tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "response_normal = base_model.generate(**inputs_normal, max_new_tokens=50)\n",
        "print(\"游몱 Response of the base model:\" , base_tokenizer.decode(response_normal[0], skip_special_tokens=True, do_sample=True))\n",
        "\n",
        "inputs_instr = instruction_tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "response_instr = instruction_model.generate(**inputs_instr, max_new_tokens=50)\n",
        "print(\"游몱 Response of the instruction-tuned model: \", instruction_tokenizer.decode(response_instr[0], skip_special_tokens=True, do_sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EJtiDI2Ymj"
      },
      "source": [
        "## Obtenci칩n de probabilidades\n",
        "\n",
        "A continuaci칩n, usaremos los modelos para obtener las probabilidades de la siguiente palabra una vez procesado el texto de `example-text`, pero solo para aquellas palabras de la lista `candidate_words`.\n",
        "\n",
        "En realidad, los modelos de lengua como Llama-3.2 no trabajan necesariamente a nivel de palabra, sino que por ciertos motivos que no vienen al caso, trocean las palabras pocos frecuentes en fragmentos que s칤 son frecuentes. A los elementos del vocabulario resultantes (que pueden ser palabras completas o fragmentos de ellas) se les denomina *tokens*.\n",
        "\n",
        "Por ello, aseg칰rate de que `candidate_words` contiene palabras que s칤 est치n en el vocabulario; una manera r치pida de comprobarlo es verificar que no se devuelve 0.0 para su probabilidad, ya que es lo que hace el programa si la palabra no est치 en el vocabulario. Aunque en un principio podr칤a parecer que esto no nos permite diferenciar entre palabras que no est치n en el vocabulario y palabras que s칤 lo est치n pero tienen probabilidad nula, lo cierto es que, por las funciones matem치ticas que usa la red neuronal, la salida de las palabras poco probables ser치 un valor muy peque침o, pero nunca cero.\n",
        "\n",
        "Observa que las probabilidades se presentan en notaci칩n cient칤fica, por ejemplo, `7.59027898311615e-08`. Esto equivale a $7.59027898311615 \\times 10^{-8}$ o, lo que es lo mismo, 0.0000000759027898311615."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33c-CHdSpz4y"
      },
      "outputs": [],
      "source": [
        "example_text = \"The weather in this summer day is \"\n",
        "candidate_words = [\"hot\", \"cold\", \"nice\", \"car\"]\n",
        "\n",
        "probs_base = get_word_probs(base_model, base_tokenizer, example_text, candidate_words)\n",
        "print(\"游몱 Probabilities of the base model:\", probs_base)\n",
        "\n",
        "instruction_text = \"Complete the sentence: '\" + example_text + \"'\"\n",
        "probs_instr = get_word_probs(instruction_model, instruction_tokenizer, instruction_text, candidate_words)\n",
        "print(\"游몱 Probabilities of the instruction-tuned model:\", probs_instr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3S1nd2m_juO"
      },
      "source": [
        "## Generaci칩n de respuestas\n",
        "\n",
        "Por 칰ltimo, generaremos m치s salidas. En el caso del modelo que sigue instrucciones, usaremos la plantilla que se us칩 durante su entrenamiento para obtener mejores resultados.\n",
        "\n",
        "Para las personas muy observadoras, en el c칩digo anterior hemos usado por separado el *tokenizador* (que obtiene los *tokens* asociados a las palabras) y el modelo en s칤. En el c칩digo siguiente, usaremos un *pipeline* que simplifica el c칩digo para obtener la salida del modelo, pero no nos permite obtener de forma directa las probabilidades de salida que nos interesaban antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPvSj3rj5AF5"
      },
      "outputs": [],
      "source": [
        "output = base_pipe(\"The key to life is\", max_new_tokens=50, do_sample=True)\n",
        "print(\"游몱 Response of the base model: \", output[0][\"generated_text\"])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "outputs = instruction_pipe(messages, max_new_tokens=50, do_sample=True)\n",
        "print(\"游몱 Response of the instruction-tuned model: \", outputs[0][\"generated_text\"][-1])   # print only new output\n",
        "\n",
        "# call apply_chat_template if not using the pipeline:\n",
        "# prompt = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
