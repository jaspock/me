{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech\n",
    "\n",
    "pytorch backends:\n",
    "https://github.com/facebookresearch/demucs/issues/570\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchaudio in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.28.1)\n",
      "Requirement already satisfied: lit in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from triton==2.0.0->torch) (17.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pip install torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri _data/yes_no/waves_yesno/0_0_0_1_0_0_0_1.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     figure\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     21\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m waveform, sample_rate, label \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m plot_specgram(waveform, sample_rate, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m IPython\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mAudio(waveform, rate\u001b[38;5;241m=\u001b[39msample_rate)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torchaudio/datasets/yesno.py:85\u001b[0m, in \u001b[0;36mYESNO.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m        labels\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m fileid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_walker[n]\n\u001b[0;32m---> 85\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torchaudio/datasets/yesno.py:65\u001b[0m, in \u001b[0;36mYESNO._load_item\u001b[0;34m(self, fileid, path)\u001b[0m\n\u001b[1;32m     63\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m fileid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     64\u001b[0m file_audio \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, fileid \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m waveform, sample_rate, labels\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torchaudio/_backend/utils.py:203\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    118\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    119\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.9/site-packages/torchaudio/_backend/utils.py:115\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri _data/yes_no/waves_yesno/0_0_0_1_0_0_0_1.wav and format None."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "_SAMPLE_DIR = \"_data\"\n",
    "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "\n",
    "dataset = torchaudio.datasets.YESNO(YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    figure, ax = plt.subplots()\n",
    "    ax.specgram(waveform[0], Fs=sample_rate)\n",
    "    figure.suptitle(title)\n",
    "    figure.tight_layout()\n",
    "\n",
    "i = 1\n",
    "waveform, sample_rate, label = dataset[i]\n",
    "plot_specgram(waveform, sample_rate, title=f\"Sample {i}: {label}\")\n",
    "IPython.display.Audio(waveform, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.49M/4.49M [00:03<00:00, 1.54MB/s]\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Batch: \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Función para visualizar espectrograma\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_spectrogram\u001b[39m(spectrogram):\n",
      "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (spectrograms, targets, input_lengths, target_lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/speech.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         output \u001b[39m=\u001b[39m model(spectrograms)  \u001b[39m# (batch, time, n_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:85\u001b[0m, in \u001b[0;36mYESNO.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m        labels\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m fileid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_walker[n]\n\u001b[0;32m---> 85\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_item(fileid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_path)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/datasets/yesno.py:65\u001b[0m, in \u001b[0;36mYESNO._load_item\u001b[0;34m(self, fileid, path)\u001b[0m\n\u001b[1;32m     63\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m fileid\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     64\u001b[0m file_audio \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, fileid \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(file_audio)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m waveform, sample_rate, labels\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:203\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m    118\u001b[0m     uri: Union[BinaryIO, \u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    119\u001b[0m     frame_offset: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     backend: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[1;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     backend \u001b[39m=\u001b[39m dispatcher(uri, \u001b[39mformat\u001b[39;49m, backend)\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[39mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torchaudio/_backend/utils.py:115\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m backend\u001b[39m.\u001b[39mcan_decode(uri, \u001b[39mformat\u001b[39m):\n\u001b[1;32m    114\u001b[0m         \u001b[39mreturn\u001b[39;00m backend\n\u001b[0;32m--> 115\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find appropriate backend to handle uri \u001b[39m\u001b[39m{\u001b[39;00muri\u001b[39m}\u001b[39;00m\u001b[39m and format \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri data/waves_yesno/1_1_0_0_0_1_1_1.wav and format None."
     ]
    }
   ],
   "source": [
    "import os\n",
    "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from torch.nn import Module, Conv2d, Linear, TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definición del modelo\n",
    "class SpeechRecognitionModel(Module):\n",
    "    def __init__(self, num_classes, input_size=128, num_heads=4, num_layers=3, hidden_size=256):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        # Capas convolucionales para reducir la dimensionalidad\n",
    "        self.conv1 = Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2))\n",
    "        self.conv2 = Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        # Transformer para el procesamiento secuencial\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "\n",
    "        # Capa lineal para el mapeo a la salida\n",
    "        self.fc = Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.permute(2, 0, 1)  # Cambiar a formato (seq_len, batch, features)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Cargar y procesar datos\n",
    "def data_processing(data):\n",
    "    spectrogram_transform = MelSpectrogram()\n",
    "    waveform, _, utterances, _, _, _ = zip(*data)\n",
    "    spectrograms = [spectrogram_transform(w).squeeze(0).transpose(0, 1) for w in waveform]\n",
    "    input_lengths = [len(s) for s in spectrograms]\n",
    "    target_lengths = [len(u) for u in utterances]\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    targets = [torch.tensor([ord(c) for c in u]) for u in utterances]\n",
    "    targets = nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    return spectrograms, targets, input_lengths, target_lengths\n",
    "\n",
    "# Configuración\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "num_classes = 29  # 26 letras + espacio, apóstrofe y caracter en blanco\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Conjuntos de datos y cargadores\n",
    "#train_dataset = LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
    "#test_dataset = LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
    "train_dataset = torchaudio.datasets.YESNO(\"./data\", download=True)\n",
    "test_dataset = torchaudio.datasets.YESNO(\"./data\", download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_processing)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_processing)\n",
    "\n",
    "# Instanciar modelo y optimizador\n",
    "model = SpeechRecognitionModel(num_classes=num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (spectrograms, targets, input_lengths, target_lengths) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "            loss = criterion(output, targets, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Función para visualizar espectrograma\n",
    "def plot_spectrogram(spectrogram):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spectrogram.log2(), aspect='auto', origin='lower', \n",
    "               cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Reproducir un archivo de audio\n",
    "def play_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "\n",
    "# Prueba de visualización y reproducción de un archivo\n",
    "test_audio_path = \"./data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac\"\n",
    "waveform, sample_rate = torchaudio.load(test_audio_path)\n",
    "spectrogram = MelSpectrogram()(waveform)\n",
    "plot_spectrogram(spectrogram[0])\n",
    "play_audio(test_audio_path)\n",
    "\n",
    "# Evaluación del modelo con un solo archivo de audio\n",
    "def evaluate(model, audio_path):\n",
    "    model.eval()\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "    spectrogram = MelSpectrogram()(waveform).unsqueeze(0).transpose(2, 3)\n",
    "    with torch.no_grad():\n",
    "        output = model(spectrogram)\n",
    "        output = output.permute(1, 0, 2)  # Reordenar a (time, batch, n_class)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        output = output.transpose(0, 1).squeeze(0)\n",
    "        return ''.join([chr(o + 96) for o in output if o != 28])  # Convertir a texto\n",
    "\n",
    "# Uso de evaluate para probar un archivo\n",
    "print(evaluate(model, test_audio_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
