{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9p6ZzUVGevf"
      },
      "source": [
        "# A softmax regressor for topic classification\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/softmax.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Notebook and code written by Juan Antonio PÃ©rez in 2024.\n",
        "\n",
        "We will build a softmax regressor that performs sentence topic classification. It will be trained with sentences belonging to different topics and then evaluated to classifiy new sentences. As the softmax regressor expects a single vector (or a batch of vectors) as input, we will transform every sentence into an embedding vector. You probably don't know how to perform this transformation yet, so for now we will rely on a library that performs the task for us. \n",
        "\n",
        "It is assumed that you are already familiar with the basics of PyTorch, but at a absolute beginner level only. This notebook complements a [learning guide](https://dlsi.ua.es/~japerez/materials/transformers/intro/) based on studying the math behind the models by reading the book \"[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)\" (3rd edition) by Jurafsky and Martin. It is part of a series of notebooks which are supposed to be incrementally studied, so make sure you follow the right order. If your learning is being supervised by a teacher, follow the additional instructions that you may have received. Although you may use a GPU environment to execute the code, the computational requirements for the default settings are so low that you can probably run it on CPU.\n",
        "\n",
        "Unlike the notebook on the logistic regressor, here we will use PyTorch functions to build the model and train it, instead of relying on a low level implementation which will be harder and harder to maintain as we build more complex models. But first, let's proceed with module installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dObaT4SGwQx",
        "outputId": "b9f75fd2-6315-436e-c936-9ca484fcbabd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch transformers numpy sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    \n",
        "set_seed(42)  # to ensure reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rWSKdmyGzTm"
      },
      "source": [
        "## Obtaining the data\n",
        "\n",
        "### Sentence embeddings\n",
        "\n",
        "The library that performs the transformation of a sentence into a vector (embedding) is called `sentence_transformers`. It can work with a number of different models available on the HuggingFace hub. Each model has specific features such as number of parameters, size of the resulting embeddings, supported languages, etc. The particular model is not relevant for our objectives here, so let's use one of the smallest (and fastest) called [gte-small](https://huggingface.co/thenlper/gte-small). It works for English and returns sentence embeddings of size 384. See it in action in the following code outputing embeddings for two sentences. \n",
        "\n",
        "The resulting embeddings have some interesting properties; for example, two sentences with similar meanings will produce *similar* embeddings. In natural language processing we will measure this similarity with metrics such as the *cosine similarity* or a less compute-intensive alternative such as the *dot product* (also called scalar product). We could do `print(embeddings[0])` to print the whole tensor, but it would print a lot of values (actually, 384), so let's print the first 3 values as a subtensor so that you can see that sentences have been transformed into a bunch of numbers. Don't worry too much about the particular values of the cosine similarity (they are values between -1 and 1); just focus on the fact that similar sentences will have closer embeddings and cosine similarities closer to 1.\n",
        "\n",
        "The first run will take a while as the model is downloaded from the HuggingFace hub. Subsequent runs will be faster as the model will be cached locally.\n",
        "\n",
        "Recall that this library is just an excuse to build a softmax regressor, so its use is a secondary objective. Later on we will learn how to build our own embeddings from scratch.\n",
        "\n",
        "ðŸ“˜ *Documentation:* [sentence_transformers.SentenceTransformer.encode](https://www.sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2M9W90hGzkk",
        "outputId": "899d7fab-e742-4bc3-a55b-109d98abc350"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "sentences = ['That is a happy person', 'That is a very happy person', 'That is a sad person', 'My drawing was not a picture of a hat']\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_embedding = SentenceTransformer('thenlper/gte-small', device=device)\n",
        "embeddings = model_embedding.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "print(f\"embeddings.shape={embeddings.shape}\")\n",
        "print(f\"embeddings[0][:3]={embeddings[0][:3]}\")  # first 3 dimensions of the first sentence\n",
        "\n",
        "cos = nn.CosineSimilarity(dim=0) \n",
        "# works on dim=1 by default (assuming two batches of tensors to compare, but there is only dim=0 in our case)\n",
        "\n",
        "print(f\"Cosine similarity between '{sentences[0]}' and '{sentences[1]}' is {cos(embeddings[0], embeddings[1]).item():.3f}\")\n",
        "print(f\"Cosine similarity between '{sentences[0]}' and '{sentences[2]}' is {cos(embeddings[0], embeddings[2]).item():.3f}\")\n",
        "print(f\"Cosine similarity between '{sentences[0]}' and '{sentences[3]}' is {cos(embeddings[0], embeddings[3]).item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the dataset\n",
        "\n",
        "After seeing an example of how to use the `sentence_transformers` library to transform sentences into embeddings, let's proceed with the preparation of the data. As our objective is not to train a real useful system but to learn how to use PyTorch, we will use an extremely tiny dataset comprised of a few sentences belonging to 3 different topics.\n",
        "\n",
        "We use `sentence_transformers.SentenceTransformer.encode` in order to transform the train and test sentences into embeddings. The results are PyTorch tensors thanks to the `to_tensor` argument, so we can use them directly in our PyTorch model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences_train = [\n",
        "    # politics:\n",
        "    'The election results were surprising',\n",
        "    'The new policy on healthcare was announced',\n",
        "    'The government passed a new education reform',\n",
        "    # sports:\n",
        "    'The soccer team won the championship',\n",
        "    'The Olympics will be held next year',\n",
        "    'She won the gold medal in swimming',\n",
        "    # culture:\n",
        "    'The museum exhibited contemporary art',\n",
        "    'The film festival was a huge success',\n",
        "    'The concert was sold out'\n",
        "]\n",
        "\n",
        "labels_train = [0, 0, 0, 1, 1, 1, 2, 2, 2]  # 0: politics, 1: sports, 2: culture\n",
        "\n",
        "sentences_test = [\n",
        "    'A new law was proposed',\n",
        "    'The basketball game was exciting',\n",
        "    'The opera was a beautiful experience'\n",
        "]\n",
        "\n",
        "labels_test = [0, 1, 2]\n",
        "\n",
        "embeddings_train = model_embedding.encode(sentences_train, convert_to_tensor=True)\n",
        "embeddings_test = model_embedding.encode(sentences_test, convert_to_tensor=True)\n",
        "labels_train = torch.tensor(labels_train).to(device)\n",
        "labels_test = torch.tensor(labels_test).to(device)\n",
        "\n",
        "print(f\"type(embeddings_train)={type(embeddings_train)}\")\n",
        "print(f\"embeddings_train.shape={embeddings_train.shape}\")\n",
        "print(f\"embeddings_train[0][:3]={embeddings_train[0][:3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model definition\n",
        "\n",
        "To define our model with PyTorch we write a class that inherits from `torch.nn.Module`. We define the layers and parameters of the model in the constructor (`__init__`) and the forward pass in the `forward` method. The forward pass is the method that will be called to obtain the output of a model given an input. In our case, the input will be a mini-batch of sentence embeddings and the output will be a vector of logits of size 3 (one value for each topic). The softmax function will then be applied to the output vector to obtain a probability distribution over the 3 topics.\n",
        "\n",
        "In the code, we define a class `SoftmaxRegressor` which extends `torch.nn.Module`, the fundamental building block for neural networks in PyTorch. The `__init__` method has two parameters: `input_size` (size of the input features without the batch dimension; in our case, dimensionality of the embeddings) and `num_classes` (number of output classes). The call to `super().__init__()` within this method is essential as it initializes the parent class, which implements most of the functionality of our model. A linear layer is created using `torch.nn.Linear` which applies a linear transformation to the incoming data. This linear layer is essentially a matrix of weights that gets multiplied with the inputs, followed by the addition of a bias term, all of them registered as parameters of the model whose values will be updated during training. The linear model assumes a first dimension corresponding to the batch size, so the input features are expected to be of size `(batch_size, input_size)`. The output of the linear layer will be of size `(batch_size, num_classes)`.\n",
        "\n",
        "The `forward` method outlines the forward pass of the neural network. Here, the output of the linear layer is passed through the `torch.nn.functional.log_softmax` function. This function applies a softmax activation followed by a logarithm, producing the logarithmic probabilities of the classes. The `dim=1` argument in `log_softmax` specifies that the softmax should be applied across the columns (moving from the first to the last column), calculating a different softmax for each row in the batch. The `log_softmax` function is used instead of the `softmax` function because it is numerically more stable. Remember the fundamental concepts here: the constructor defines the layers and parameters of the model, and the forward function defines how the input is transformed into the output.\n",
        "\n",
        "ðŸ“˜ *Documentation:* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), [torch.nn.functional.log_softmax](https://pytorch.org/docs/stable/generated/torch.nn.functional.log_softmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SoftmaxRegressor(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return F.log_softmax(self.linear(X), dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model instantiation\n",
        "\n",
        "When training our model, our recipe will contain three fundamental ingredients: the model, the loss function, and the optimizer. A fourth one is the learning rate scheduler, which is not used in this example. The scheduler adjusts the learning rate during training, usually rising it at the beginning and then reducing it as the training progresses. This way, model's convergence is more easily achieved.\n",
        "\n",
        "In the code, we start by creating an instance of our model and moving it to the device where we will perform the computations. The object `criterion` of class `torch.nn.NLLLoss` does not only set up the loss function, but also contains the logic to compute the derivative of the loss function with respect to the model parameters. The `optimizer` object of class `torch.optim.SGD` implements the stochastic gradient descent algorithm, which is used to update the model parameters during training based on the gradient of the loss function. The function `torch.nn.Module.parameters` inherited by our model class returns an iterator over the model parameters (weights and bias of the linear layer in our case). There are more advanced optimizers available in PyTorch, but we will stick to the basic one for now.\n",
        "\n",
        "In PyTorch, `torch.nn.CrossEntropyLoss`, `torch.nn.BCELoss`, and `torch.nn.NLLLoss` are loss functions used in different classification contexts, each with specific requirements for their input types. `CrossEntropyLoss` is widely used for multi-class classification tasks and is designed to take *logits* (the raw, unnormalized scores outputted by the final layer of the network) as its inputs. Conveniently, `CrossEntropyLoss` does not require the target labels to be one-hot encoded, and expects them to be represented as class indices, which is usually how we will have them. Internally, this function applies the softmax function to the logits and then computes the log loss. In binary classification scenarios, `BCELoss` (binary cross-entropy loss) is the go-to choice. This function expects the outputs of your model to be probabilities; the target labels for `BCELoss` should be in the same format as the model outputs, i.e., probabilities, not indices. Finally, `NLLLoss` (negative log likelihood loss) is used in multiclass classification tasks, but expects log-probabilities in the output, which requires applying a `log_softmax` function to your model's final layer outputs. The target labels for `NLLLoss` should also be class indices, not one-hot encoded vectors. In summary, `CrossEntropyLoss` and `NLLLoss` are used for multi-class classification and expect class indices as targets, while `BCELoss` is for binary classification and expects probabilities.\n",
        "\n",
        "ðŸ“˜ *Documentation:* [torch.nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_size = embeddings_train.size(1)  # input embedding size to avoid magic numbers\n",
        "num_classes = 3  # number of topics\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = SoftmaxRegressor(input_size, num_classes)\n",
        "model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "The training loop is similar to the one you studied in the previous notebook, but here we delegate in PyTorch the computation of the gradients and the update of the model parameters. The name of the necessary PyTorch functions somehow match those of our own functions in the previous notebook, so it will be easy to understand what is going on. \n",
        "\n",
        "Notice how the code seems to call `model` and `criterion` which have been defined as objects before, not as functions. This is possible because the classes of these objects implement the `__call__` method, which allows them to be called as if they were functions. Python allows the programmer to define a number of these *dunder* (for *double underscores*) magic methods. In this case, `__call__` is defined in `nn.Module` and calls the overriden `forward` method in the child class (our `SoftmaxRegressor` class in this case).\n",
        "\n",
        "The function `torch.Tensor.backward` computes the gradient of the loss with respect to the model parameters. The function `torch.optim.Optimizer.step` updates the model parameters based on the computed gradients. The function `torch.optim.Optimizer.zero_grad` clears the gradients of all optimized parameters. This is not done automatically by PyTorch because we might want to accumulate gradients from multiple batches before updating the parameters; this way, we could emulate epoch-based training by accumulating gradients from all batches and then updating the parameters. Here, however, we update the parameters after each mini-batch.\n",
        "\n",
        "Regarding the call to `torch.nn.Module.train`, this is not strictly necessary in our case, but it is good practice to call it before training. This function sets the model in training mode, which is important for some layers that we may have in our models, such as `Dropout` and `BatchNorm` which behave differently during training and evaluation. Alternatively, the function `torch.nn.Module.eval` sets the model in evaluation mode as we will see later.\n",
        "\n",
        "ðŸ“˜ *Documentation:* [torch.optim.Optimizer.zero_grad](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html), [torch.Tensor.backward](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html), [torch.optim.Optimizer.step](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_steps = 10000\n",
        "eval_steps = 1000\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "for i in range(training_steps):\n",
        "    outputs = model(embeddings_train)\n",
        "    loss = criterion(outputs, labels_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % eval_steps == 0:\n",
        "        print(f'Step [{i}/{training_steps}], loss: {loss.item():.4f}')\n",
        "        losses.append(loss.item())\n",
        "\n",
        "print(f'Step [{training_steps}/{training_steps}], loss: {loss.item():.4f}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot training loss:\n",
        "plt.plot(losses, label='Cross-entropy training loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Evolution of loss during training')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "When computing accuracy over a test set in PyTorch, using `with torch.no_grad()` is a good practice for optimizing performance. The primary function of `no_grad` is to disable gradient tracking, which is essential during the training phase but unnecessary during inference or evaluation. By turning off gradient computation, memory usage is reduced and computations accelerated. Within the `torch.no_grad` context, the result of every operation will have `requires_grad=False`, meaning that the PyTorch autograd engine will not track these operations for gradient calculation. It is important to note that `no_grad` is orthogonal to the `eval` and `train` methods; while `eval` sets the model to evaluation mode (affecting layers like dropout and batch normalization), `no_grad` is specifically focused on disabling gradient computation.\n",
        "\n",
        "The accuracy is computed in a similar way as in the previous notebook, but choosing as predicted label the one with the highest probability. \n",
        "\n",
        "ðŸ“˜ *Documentation:* [torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html), [torch.max](https://pytorch.org/docs/stable/generated/torch.max.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = len(embeddings_test)\n",
        "    outputs = model(embeddings_test)\n",
        "    print(f'outputs={outputs}')\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "    print(f'predicted={predicted}')\n",
        "    correct += (predicted == labels_test).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on test data: {accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model saving and loading\n",
        "\n",
        "The function `torch.save` saves the model parameters to a file. The function `torch.load` loads the model parameters from a file. The file extension is usually `.pt` or `.pth`. There are two possibilities for model saving depending on whether we want to continue training the model or not. During training, a large amount of information beyond the model parameters may be stored by, for example, the optimizer. If we want to continue training the model from the current point, all these must be saved too with `torch.save(model, path)`. If we just want to redistribute the model for inference, we can save only the model parameters with `model.state_dict` as in the following code. The state dictionary of a model is a Python dictionary object that maps each layer to its parameter tensor. \n",
        "\n",
        "ðŸ“˜ *Documentation:* [torch.save](https://pytorch.org/docs/stable/generated/torch.save.html), [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html), [saving and loading](https://pytorch.org/tutorials/beginner/saving_loading_models.html) models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pth')\n",
        "loaded_model = SoftmaxRegressor(input_size, num_classes)\n",
        "loaded_model.load_state_dict(torch.load('model.pth'))\n",
        "loaded_model.eval()\n",
        "# ...\n",
        "# now you can use loaded_model to make predictions..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "If your learning path is supervised by a teacher, they may have provided you with additional instructions on how to proceed with the exercises.\n",
        "\n",
        "âœŽ Modify the code so that each mini-batch contains a single sentence embedding instead of a batch of embeddings. Display the loss evolution during training and compare it to the epoch-based training. What do you observe? Why do you think this happens?\n",
        "\n",
        "âœŽ Modify the code of the logistic regressor notebook so that it uses a softmax regressor with two classes as implemented in the current notebook instead of a logistic regressor. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
