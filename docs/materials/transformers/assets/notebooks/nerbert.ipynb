{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Named entities recognition with transformers\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio PÃ©rez in 2024.\n",
        "\n",
        "This notebook presents \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "\n",
        "def make_batch(input_sentences, output_tags, word_index, tag_index, max_len, batch_size, device):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    data_cycle = itertools.cycle(zip(input_sentences, output_tags))\n",
        "\n",
        "    # to-do: adjust T to be minimum of the actual max length of the batch or max_len\n",
        "\n",
        "    while True:\n",
        "        for s,t in data_cycle:\n",
        "            words = s.split()\n",
        "            tags = t.split()\n",
        "            assert len(words) == len(tags)\n",
        "            inputs = [word_index[n] for n in words]\n",
        "            inputs = inputs + [0] * (max_len - len(inputs))  # padded inputs\n",
        "            tags = [tag_index[n] for n in tags]\n",
        "            tags = tags + [0] * (max_len - len(tags))  # padded outputs\n",
        "            input_batch.append(inputs)\n",
        "            output_batch.append(tags)\n",
        "\n",
        "            if len(input_batch) == batch_size:\n",
        "                yield torch.LongTensor(input_batch, device=device), torch.LongTensor(output_batch, device=device)\n",
        "                input_batch = []\n",
        "                output_batch = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input vocab size: 31\n",
            "output vocab size: 11\n"
          ]
        }
      ],
      "source": [
        "input_sentences = [\n",
        "    \"The cat sat on the mat .\",\n",
        "    \"I love eating pizza .\",\n",
        "    \"John is running in the park .\",\n",
        "    \"She gave him a beautiful gift .\",\n",
        "    \"They are playing soccer together .\",\n",
        "    \"The cat is eating pizza in the park .\"\n",
        "]\n",
        "\n",
        "output_tags = [\n",
        "    \"DET NOUN VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB VERB NOUN PUNCT\",\n",
        "    \"PROPN AUX VERB ADP DET NOUN PUNCT\",\n",
        "    \"PRON VERB PRON DET ADJ NOUN PUNCT\",\n",
        "    \"PRON AUX VERB NOUN ADV PUNCT\",\n",
        "    \"DET NOUN AUX VERB NOUN ADP DET NOUN PUNCT\"\n",
        "]\n",
        "\n",
        "word_list = list(set(\" \".join(input_sentences).split()))\n",
        "word_index = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "special_tokens = len(word_index) \n",
        "for step, w in enumerate(word_list):\n",
        "    word_index[w] = step + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "input_vocab_size = len(word_index)\n",
        "tag_list = list(set(\" \".join(output_tags).split()))\n",
        "tag_index = {'[PAD]': 0}  # padding index must be 0\n",
        "for step, t in enumerate(tag_list):\n",
        "    tag_index[t] = step + 1\n",
        "index_tag = {i:t for i, t in enumerate(tag_index)}\n",
        "output_vocab_size = len(tag_index)\n",
        "print(\"input vocab size: %d\" % input_vocab_size)\n",
        "print(\"output vocab size: %d\" % output_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.10M\n",
            "Step [0/1000], loss: 2.4697\n",
            "Step [100/1000], loss: 1.0569\n",
            "Step [200/1000], loss: 0.1634\n",
            "Step [300/1000], loss: 0.0388\n",
            "Step [400/1000], loss: 0.0181\n",
            "Step [500/1000], loss: 0.0108\n",
            "Step [600/1000], loss: 0.0087\n",
            "Step [700/1000], loss: 0.0069\n",
            "Step [800/1000], loss: 0.0066\n",
            "Step [900/1000], loss: 0.0058\n",
            "Step [1000/1000], loss: 0.0061\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 3\n",
        "max_len = 12\n",
        "lr = 0.001\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = EncoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, input_vocab_size=input_vocab_size, output_vocab_size=output_vocab_size, \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=training_steps, epochs=1, anneal_strategy='cos')\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "for inputs, outputs in make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, \n",
        "                                    tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if step % eval_steps == 0:\n",
        "        print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==training_steps):\n",
        "        break\n",
        "\n",
        "print(f'Step [{step}/{training_steps}], loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[20, 17, 24,  6,  9, 12, 21,  0,  0,  0,  0,  0],\n",
            "        [27, 15,  4, 22, 21,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [28, 11,  5, 10,  9, 26, 21,  0,  0,  0,  0,  0]]) tensor([[ 1,  4,  6,  9,  1,  4,  8,  0,  0,  0,  0,  0],\n",
            "        [10,  6,  6,  4,  8,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 2,  5,  6,  9,  1,  4,  8,  0,  0,  0,  0,  0]])\n",
            "Input:\n",
            " The cat sat on the mat . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "I love eating pizza . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "John is running in the park . [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Prediction: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT NOUN NOUN NOUN NOUN NOUN\n",
            "PRON VERB VERB NOUN PUNCT NOUN NOUN NOUN NOUN NOUN NOUN NOUN\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT NOUN NOUN NOUN NOUN NOUN\n",
            "Target: \n",
            " DET NOUN VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PRON VERB VERB NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "PROPN AUX VERB ADP DET NOUN PUNCT [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "# predict tags\n",
        "model.eval()\n",
        "inputs, outputs = make_batch(input_sentences=input_sentences, output_tags=output_tags, word_index=word_index, tag_index=tag_index, max_len=max_len, batch_size=batch_size, device=device).__next__()\n",
        "print(inputs,outputs)\n",
        "logits = model(inputs)\n",
        "_, indices = torch.max(logits, dim=-1)\n",
        "predict_tags, true_tags, input_words = [], [], []  # 3 lists are required, not one\n",
        "for step in range(batch_size):\n",
        "    predict_tags.append(\" \".join([index_tag[each.item()] for each in indices[step]]))\n",
        "    true_tags.append(\" \".join([index_tag[each.item()] for each in outputs[step]]))\n",
        "    input_words.append(\" \".join([index_word[each.item()] for each in inputs[step]]))\n",
        "print(\"Input:\\n\", \"\\n\".join(input_words))\n",
        "print(\"Prediction: \\n\", \"\\n\".join(predict_tags))\n",
        "print(\"Target: \\n\", \"\\n\".join(true_tags))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
