{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Entrenamiento de un modelo de lengua extremadamente simple\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/simple-lm.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "<a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Cuaderno y código escrito por Juan Antonio Pérez en 2025. Originalmente inspirado en el código de Tae Hwan Jung (@graykode) en el [tutorial de NLP](https://github.com/graykode/nlp-tutorial).\n",
        "\n",
        "Este cuaderno presenta un modelo de lengua intencionadamente simple basado en unas redes neuronales llamadas *feed-forward neural networks* o también *perceptrones multicapa*. Está tan simplificado que será entrenado con oraciones muy cortas y solo podrá predecir la siguiente palabra a partir de las dos palabras anteriores. Además, nuestro conjunto de entrenamiento será tan pequeño que el modelo probablemente lo memorizará. Esto implica que la red no podrá generalizar a oraciones que no haya visto durante el entrenamiento y, como resultado, la probaremos con las mismas oraciones utilizadas para entrenar; esto constituye una muy mala práctica, que utilizaremos aquí únicamente por razones de simplicidad. A pesar de todas estas limitaciones, este cuaderno será útil para ilustrar los conceptos básicos del modelado del lenguaje con redes neuronales simples.\n",
        "\n",
        "Para ejecutar este cuaderno de Python puedes usar un entorno como Google Colab, que te permitirá ejecutar el código en la nube sin necesidad de instalar nada en tu ordenador. Desde el menú *Entorno de ejecución* puedes seleccionar *Ejecutar todas* para ejecutar todas las celdas de código. El consumo de recursos es tan reducido que el entorno de ejecución que uses no necesita GPU: asegúrate en *Entorno de ejecución* / *Cambiar tipo de entorno de ejecución* de que no estás gastando recursos innecesarios y reserva las pocas horas gratuitas de GPU que tienes para tareas más exigentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_Xr2BNs7KO"
      },
      "source": [
        "## Configuración del entorno\n",
        "\n",
        "La siguiente celda configura establece una semilla para el generador de números aleatorios de forma que los resultados sean siempre los mismos entre distintas ejecuciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE-9gggqs7KS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# set before importing pytorch to avoid all non-deterministic operations on GPU\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "set_seed(42)  # to ensure reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_vEAAo3s7KT"
      },
      "source": [
        "## Datos de entrenamiento y vocabulario\n",
        "\n",
        "La siguiente celda define las frases del conjunto de entrenamiento. El vocabulario se construye a partir de todas las palabras que hay en estas frases. La variable `window_size` define el número de palabras que se usarán para predecir la siguiente palabra, pero no es necesario que la modifiques. La función `make_batch` genera un bloque de datos de entrenamiento en el que las entradas son los enteros que representan las palabras previas y la salida deseada se representa con el índice de la siguiente palabra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoFOvCz5s7KU"
      },
      "outputs": [],
      "source": [
        "window_size = 2\n",
        "\n",
        "sentences = [\"let's promote peace\", \"let's ban war\", \"let's teach compassion\", \"let's build a better world\"]\n",
        "\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "word_index = {w: i for i, w in enumerate(word_list)}\n",
        "index_word = {i: w for i, w in enumerate(word_list)}\n",
        "vocab_size = len(word_index)\n",
        "\n",
        "print(f\"word_index = {index_word}\")\n",
        "\n",
        "def make_batch():\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()  # space tokenizer\n",
        "        for i in range(len(words) - 2):\n",
        "            input = [word_index[words[i]], word_index[words[i+1]]]  # indices of two consecutive words\n",
        "            target = word_index[words[i+2]]  # index of the next word\n",
        "            input_batch.append(input)\n",
        "            target_batch.append(target)\n",
        "\n",
        "    return input_batch, target_batch\n",
        "\n",
        "inputs, targets = make_batch()\n",
        "print(f\"inputs = {inputs}\")\n",
        "print(f\"targets = {targets}\")\n",
        "print(f\"inputs = {[[index_word[i] for i in x] for x in inputs]}\")\n",
        "print(f\"targets = {[index_word[i] for i in targets]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMAds5LPs7KW"
      },
      "source": [
        "## Definición del modelo\n",
        "\n",
        "Aunque no vas a entender completamente el código de la siguiente celda, en ella se definen los módulos de la librería PyTorch que componen nuestro modelo. La función `nn.Embedding` permite convertir los índices de las palabras en vectores de embeddings de tamaño `embedding_size`. Nuestro modelo tiene dos matrices creadas por la función `nn.Linear`. La primera, `W`, es con la que se multiplica la entrada para obtener un nuevo vector. Este vector intermedio se multiplica por la segunda matriz, `U`, para obtener los valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDG0wcTfs7KW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, window_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.window_size = window_size\n",
        "        self.C = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.W = nn.Linear(window_size * embedding_size, hidden_size, bias=True)\n",
        "        self.U = nn.Linear(hidden_size, vocab_size, bias=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.C(X)\n",
        "        X = X.view(-1, self.window_size * self.embedding_size)\n",
        "        X = F.relu(self.W(X))\n",
        "        X = self.U(X)\n",
        "        return X  # return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfLAjd67s7KY"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "El siguiente código es muy habitual cuando se entrenan redes neuronales. El modelo se crea en la llamada a `NNLM` (que es la clase que hemos definido antes) y se guarda en la variable `model`. A continuación, se define la función de pérdida en la variable `criterion` y el mecanismo de actualización de los parámetros en la variable `optimizer`.\n",
        "\n",
        "Después de obtener los datos en la llamada a `make_batch`, el programa entra en un bucle de entrenamiento durante `training_steps` pasos. En cada paso del bucle, se computa la salida de la red neuronal en la variable `output` tras pasar al modelo los datos de entrada guardados en `input_batch`; a continuación, se calcula la pérdida u error entre la salida obtenida en `output` y la salida deseada que está en `target_batch`; después, la llamada a `backward` calcula la información necesaria para determinar el cambio a realizar en cada parámetro, actualización que se lleva efectivamente a cabo en la llamada a `optimizer.step`.\n",
        "\n",
        "Cada cierto número de pasos, se imprime el valor de la función de pérdida que idealmente se irá reduciendo a medida que el modelo aprenda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmdhoVEgs7KZ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "hidden_size = 4\n",
        "embedding_size = 2\n",
        "training_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.005\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = NNLM(vocab_size, embedding_size, window_size, hidden_size)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=training_steps)\n",
        "\n",
        "input_batch, target_batch = make_batch()\n",
        "input_batch = torch.LongTensor(input_batch)\n",
        "target_batch = torch.LongTensor(target_batch)\n",
        "\n",
        "lr_history = []  # learning rate history\n",
        "model.train()\n",
        "for i in range(training_steps):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_batch)\n",
        "    loss = criterion(output, target_batch)\n",
        "    if i % eval_steps == 0:\n",
        "        print(f'Step [{i}/{training_steps}], loss: {loss.item():.4f}')\n",
        "    lr_history.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # scheduler must be called after optimizer\n",
        "\n",
        "print(f'Step [{training_steps}/{training_steps}], loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDVfcVZas7Kb"
      },
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "Evaluamos el modelo con sus propios datos de entrenamiento, lo cual es una práctica terrible, pero la única que tiene algo de sentido dado el tamaño extremadamente pequeño del conjunto de entrenamiento, que no permitirá que el modelo generalice a datos no vistos ni desarrolle predicciones útiles.\n",
        "\n",
        "En cada línea se imprimen dos palabras y la palabra que según el modelo entrenado tiene más probabilidad de ser la siguiente. También se imprimen las tres continuaciones con probabilidad más alta y el valor de esta.\n",
        "\n",
        "Prueba a añadir al conjunto de entrenamiento alguna frase que comparta un prefijo de dos palabras con alguna existente (por ejemplo, *let's ban guns*) y observa las probabilidades predichas para la palabra siguiente al prefijo común (es decir, para la continuación de *let's ban...*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5AzeNV-s7Kb"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predict = model(input_batch)\n",
        "    predict_max = predict.argmax(dim=1)\n",
        "\n",
        "    for i in range(len(input_batch)):\n",
        "        print([index_word[n.item()] for n in input_batch[i]], '⟶', index_word[predict_max[i].item()], end=' || ')\n",
        "        top3 = predict[i].topk(3)\n",
        "        probabilities = F.softmax(top3.values, dim=0)\n",
        "        formatted_output = ', '.join([f\"{index_word[top3.indices[j].item()]}:{probabilities[j].item():.3f}\" for j in range(3)])\n",
        "        print(formatted_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpln",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
