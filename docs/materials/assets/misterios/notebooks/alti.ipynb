{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K78XMsqZLTg-"
      },
      "source": [
        "# Inspeccionando el mecanismo de atención de un modelo de lengua\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/alti.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> <a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Cuaderno preparado por Juan Antonio Pérez en 2025. Este cuaderno permite ver qué palabras de la entrada son más relevantes para el mecanismo de atención de un modelo de lengua a la hora de apostar por la siguiente palabra. Usa para ello una técnica llamada [ALTI-Logit](https://aclanthology.org/2023.acl-long.301/). La mayor parte del código de este cuaderno está tomado del [repositorio](https://github.com/mt-upc/logit-explanations) de los autores de esta técnica.\n",
        "\n",
        "El entorno de ejecución de este cuaderno ha de tener una GPU. Si usas Google Colab, la puedes conseguir desde el menú *Entorno de ejecución* / *Cambiar tipo de entorno de ejecución*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEZpst9kJF2a"
      },
      "source": [
        "## Instalación del código necesario\n",
        "\n",
        "Los dos primeros bloques solo instalan el código del método ALTI-Logit e importan las librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqWO7ju9uUns"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/mt-upc/logit-explanations.git\n",
        "%cd logit-explanations\n",
        "# avoid installation of specific versions to speed up the process (use at your own risk):\n",
        "!sed -i -E 's/[><=!~^]+[0-9.]+//g' requirements.txt  \n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtoT_MyjuMoP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import src.utils_contributions as utils_contributions\n",
        "from src.contributions import ModelWrapper\n",
        "from extract_explanations import read_blimp_dataset, track2input_tokens, read_sva_dataset, read_ioi_dataset\n",
        "import pandas as pd\n",
        "from lm_saliency import *\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCedZoLMMzsS"
      },
      "source": [
        "\n",
        "## Selección del modelo\n",
        "\n",
        "Aunque el sistema soporta diferentes modelos de lengua de tamaño pequeño, te puedes centrar en los de la familia GPT-2.\n",
        "\n",
        "Para que funcione la descarga, has de tener una clave secreta de nombre HF_TOKEN con un valor que puedes obtener si te creas una cuenta en Hugging Face. Pregunta a tu profesor antes. Esta clave se añade desde la sección con el icono de la llave a la izquierda de este cuaderno si lo has abierto en Google Colab.\n",
        "\n",
        "La descarga de los modelos puede llevar unos minutos, especialmente la primera vez que se ejecute el cuaderno en un nuevo entorno de ejecución."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpQRCwXkTI9g"
      },
      "outputs": [],
      "source": [
        "# Currently tested for:\n",
        "#  OPT: facebook/opt-125m (bigger models use Post-LN)\n",
        "#  GPT-2: gpt2 (124M), gpt2-large (774M), gpt2-xl (1.5B)\n",
        "#  BLOOM: bigscience/bloom-560m bigscience/bloom-1b1\n",
        "name_path = 'gpt2-xl'\n",
        "model, tokenizer = utils_contributions.load_model_tokenizer(name_path)\n",
        "model_wrapped = ModelWrapper(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPkSDTWWT8v8"
      },
      "source": [
        "## Contexto y palabras objetivo y sorpresa\n",
        "\n",
        "Aquí puedes definir el texto (`text`) a utilizar con el modelo de lengua y las dos palabras siguientes que queremos comparar: por un lado, una palabra coherente en el contexto (a la que llamaremos palabra *objetivo* y que se guarda en la variable `target`) y, por otro lado, una que no lo sea (a la que llamaremos palabra *sorpresa* y que se guarda en la variable `foil`).\n",
        "\n",
        "El texto se segmenta en las diferentes palabras (a veces, unidades más pequeñas) que procesará el modelo. Cuando se imprime la lista de palabras el símbolo `Ġ` representa un espacio en blanco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG3_j-PhuMoR"
      },
      "outputs": [],
      "source": [
        "text = \"This summer, unlike the previous winter, is being extremely\"  # do not end with a space\n",
        "target = 'hot'\n",
        "foil = 'cold'  # surprise word\n",
        "\n",
        "input = text\n",
        "print('input: ' , input)\n",
        "print('target: ', target)\n",
        "print('foil: ', foil)\n",
        "\n",
        "if 'facebook/opt' in tokenizer.name_or_path:\n",
        "    # OPT tokenizer adds a BOS token at pos 0\n",
        "    CORRECT_ID = tokenizer(\" \"+ target)['input_ids'][1]\n",
        "    FOIL_ID = tokenizer(\" \"+ foil)['input_ids'][1]\n",
        "else:\n",
        "    CORRECT_ID = tokenizer(\" \"+ target)['input_ids'][0]\n",
        "    FOIL_ID = tokenizer(\" \"+ foil)['input_ids'][0]\n",
        "if CORRECT_ID == FOIL_ID:\n",
        "    raise ValueError('Same CORRECT_ID and FOIL_ID')\n",
        "\n",
        "token = [CORRECT_ID, FOIL_ID]\n",
        "pt_batch = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "input_ids = pt_batch['input_ids']\n",
        "tokenized_text = tokenizer.convert_ids_to_tokens(pt_batch[\"input_ids\"][0])\n",
        "print(tokenized_text)\n",
        "seq_len = len(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUEKs6LUPEQx"
      },
      "source": [
        "El siguiente bloque muestra las palabras siguientes más frecuentes según el modelo de lengua. Aunque lo valores expresados como *logits* no son directamente probabilidades, sí que se cumple que valores mayores de *logits* suponen probabilidades más altas, aunque no de forma proporcional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp7xWt-duMoU"
      },
      "outputs": [],
      "source": [
        "# Forward-pass\n",
        "logits, hidden_states, attentions = model_wrapped(pt_batch)\n",
        "\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1).squeeze()\n",
        "top_k = 10\n",
        "\n",
        "token_probs = probs[-1]\n",
        "sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
        "top_k_pred_t_ids = torch.topk(token_probs, k=top_k,dim=-1).indices\n",
        "top_k_pred_t_tokens = tokenizer.convert_ids_to_tokens(torch.squeeze(top_k_pred_t_ids))\n",
        "top_k_pred_t_values = torch.topk(token_probs,k=top_k,dim=-1).values\n",
        "\n",
        "for i in range(top_k):\n",
        "    print(\n",
        "        f\"Top {i}th token. Logit: {logits[0, -1, sorted_token_values[i]].item():5.3f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{tokenizer.convert_ids_to_tokens(sorted_token_values[i].item())}| String: |{tokenizer.decode(sorted_token_values[i])}|\"\n",
        "    )\n",
        "\n",
        "predicted_sentence = tokenized_text[1:] + [top_k_pred_t_tokens[0]]\n",
        "\n",
        "print(f\"CORRECT_ID token. Logit: {logits[0, -1, CORRECT_ID].item():5.3f}\")\n",
        "print(f\"FOIL_ID token. Logit: {logits[0, -1, FOIL_ID].item():5.3f}\")\n",
        "print('logits diff', logits[0, -1, CORRECT_ID] - logits[0, -1, FOIL_ID])\n",
        "\n",
        "if model_wrapped.model.config.model_type == 'opt':\n",
        "    bos_model = True\n",
        "else:\n",
        "    bos_model = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lKvbICQuMoW"
      },
      "source": [
        "## Visualización de las atenciones\n",
        "\n",
        "La matriz que se genera acontinuación representa con colores más oscuros aquellas palabras de la entrada a las que el mecanismo de atención da más importancia a la hora de considerar una determinada palabra como siguiente. La matriz se lee normalmente por filas: para la palabra de una determinada fila, los colores de las columnas indican la importancia de las palabras correspondientes.\n",
        "\n",
        "Puedes observar que cada palabra solo atiende a sí misma y a las palabras anteriores. Esto es así como consecuencia de que los modelos de lengua van calculando las atenciones y predicciones incrementalmente de izquierda a derecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqmEfeywuMoX"
      },
      "outputs": [],
      "source": [
        "logit_trans_vect_dict, logits_modules, layer_alti_data = model_wrapped.get_logit_contributions(hidden_states, attentions, token)\n",
        "contributions_mix_alti = utils_contributions.compute_alti(layer_alti_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eZywmdauMoX"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# Show ALTI input attributions to model outputs\n",
        "df = pd.DataFrame(np.array(contributions_mix_alti[-1]),columns=tokenized_text,index=tokenized_text)\n",
        "sns.heatmap(df,cmap=\"Blues\",square=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivYuV83vQ3P7"
      },
      "source": [
        "## Análisis contrastivo de las palabras relevantes\n",
        "\n",
        "El siguiente gráfico muestra la diferencia entre la atención puesta en cada palabra del contexto cuando se predice la palabra objetivo (`target`) y cuando se predice la palabra sorpresa (`foil`). La interpretación de estos valores es la siguiente:\n",
        "\n",
        "- Un valor positivo significa que esa palabra es importante para predecir la palabra objetivo, pero no para la palabra sorpresa.\n",
        "- Un valor cercano a cero significa que en ambos casos la palabra tiene una relevancia similar.\n",
        "- Un valor negativo implica que la palabra es irrelevante en la predicción de la palabra objetivo, pero relevante en la palabra sorpresa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIrNUB-JuMoY"
      },
      "outputs": [],
      "source": [
        "# Compute ALTI-Logit\n",
        "methods_decomp = ['aff_x_j'] # Logits Affine part of layer-wise decomposition\n",
        "alti_lg_dict = track2input_tokens(logit_trans_vect_dict, methods_decomp, contributions_mix_alti, token)\n",
        "alti_lg_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djcGqNgYuMoY"
      },
      "outputs": [],
      "source": [
        "# 'logit_aff_x_j_alti' ALTI-Logit explanation\n",
        "# 'logit_aff_x_j' Logit explanation\n",
        "method = 'logit_aff_x_j'\n",
        "\n",
        "# Contrastive explanation\n",
        "contrastive_contributions = (alti_lg_dict[method][0] - alti_lg_dict[method][1]).sum(0)\n",
        "\n",
        "# Add inital logit update by last postion intial embedding (see Eq. 7 paper)\n",
        "init_logits_diff = (logits_modules['init_logit'][0] - logits_modules['init_logit'][1]).to('cpu')\n",
        "contrastive_contributions[-1] += init_logits_diff\n",
        "print(contrastive_contributions.sum())\n",
        "\n",
        "# Normalization done in Kayo's work\n",
        "# Divides by the sum of the absolute values (l1) of the explanations vector\n",
        "norm = np.linalg.norm(contrastive_contributions, ord=1)\n",
        "contrastive_contributions /= norm\n",
        "explanations_list = []\n",
        "explanations_list.append(contrastive_contributions)\n",
        "# Yin and Neubig visualization (https://github.com/kayoyin/interpret-lm)\n",
        "# visualize(np.array(contrastive_contributions), tokenizer, [pt_batch[\"input_ids\"][0]], print_text=True, normalize=False)\n",
        "# Barplot visualization\n",
        "utils_contributions.plot_histogram(contrastive_contributions,tokenized_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c995e4df3e15fb19e51937be7fa850c4ca509847aa0198273373bdf21279d10e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
