{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thlU7qzxespH"
      },
      "source": [
        "# Transformer-based language models\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Code written by Juan Antonio Pérez in 2024.\n",
        "\n",
        "This notebook presents \n",
        "\n",
        "**Exercise**: use spm to tokenize the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Nos ahorramos el padding en make_batch\n",
        "- Comentar sobre desconocidas y UNK en train y test\n",
        "- Hacer versión corta con MultiHeadAttention y LN de PyTorch\n",
        "- to-do: check that original multiheadattention behaves similar to the simpler one\n",
        "- mover transformer a notebook y poner un main que ejecuta uno muy pequeño inicializado aleatoriamente\n",
        "- wget github y %run 'note.ipynb' para ejecutarlo en celda de notebook\n",
        "- hacer una clase comun a encoder y decoder (solo cambia la mascara), TransformerModule, \n",
        "- EncoderOnlyTranformer, DecoderOnlyTransformer\n",
        "- poner set_seed en los otros notebooks y evaluarlo\n",
        "- añadir a NER el lr scheduler\n",
        "- Ejercicio: que funcione con minibatches en inferencia\n",
        "- Repasar el main del BERT para hacerlo como aquí\n",
        "- usar no_grad en todas la evaluaciones\n",
        "- avisar que no usaremos prefix tuning o como se llame (supongo que hay que usarla tb durante el training)\n",
        "- ejercicio: top-p sampling\n",
        "- sustituir !pip install por %pip en todos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mini-batch preparation\n",
        "\n",
        "Notes on [reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Original code from minGPT by Andrej Karpathy\n",
        "# https://github.com/karpathy/minGPT/\n",
        "# Modifications by @jaspock\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "    \n",
        "set_seed(42)\n",
        "\n",
        "def make_language_modeling_batch(corpus, word_index, max_len, batch_size, device):\n",
        "\n",
        "    tokens = corpus.split()\n",
        "    token_indices = [word_index.get(token, word_index['[UNK]']) for token in tokens]\n",
        "    n_tokens = len(token_indices)  # number of tokens in the corpus\n",
        "    batch_token_length = batch_size * max_len  # the total number of tokens in a batch\n",
        "    assert n_tokens >= batch_token_length, f'Short corpus ({n_tokens} tokens), must be at least {batch_length} tokens long'\n",
        "\n",
        "    while True:\n",
        "        input_batch, output_batch = [], []\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            start_index = random.randint(0, n_tokens - 1)  # random start\n",
        "            end_index = start_index + max_len\n",
        "            input_seq = token_indices[start_index:end_index]\n",
        "            if end_index > n_tokens:\n",
        "                input_seq += token_indices[:end_index - n_tokens]\n",
        "            \n",
        "            # output is the same as input, except shifted one token to the right\n",
        "            output_seq = input_seq[1:] + [token_indices[end_index % n_tokens]]\n",
        "\n",
        "            input_batch.append(input_seq)\n",
        "            output_batch.append(output_seq)\n",
        "\n",
        "        yield torch.LongTensor(input_batch).to(device), torch.LongTensor(output_batch).to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import our transformer code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbformat in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (5.9.2)\n",
            "Requirement already satisfied: fastjsonschema in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nbformat) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nbformat) (4.20.0)\n",
            "Requirement already satisfied: jupyter-core in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nbformat) (5.5.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nbformat) (5.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.16.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jupyter-core->nbformat) (4.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (2.1.2)\n",
            "Requirement already satisfied: filelock in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/japerez/miniconda3/envs/tpln/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "number of parameters: 0.10M\n",
            "input: tensor([[0, 3, 0, 4, 0, 1, 2, 4, 0, 2, 0, 3, 0, 2, 2, 1]])\n",
            "logits: tensor([[[-0.1121, -0.0155, -0.1396, -0.1364, -0.0186],\n",
            "         [ 0.1585,  0.1444,  0.2344, -0.0886,  0.1354],\n",
            "         [ 0.0917, -0.0880, -0.0266, -0.1049,  0.0722],\n",
            "         [-0.0222,  0.0885, -0.1515, -0.2342, -0.0662],\n",
            "         [ 0.2581,  0.1235,  0.0467,  0.0220, -0.1452],\n",
            "         [-0.1190, -0.0858,  0.2219,  0.2356,  0.0869],\n",
            "         [-0.0240,  0.0758, -0.0711, -0.2976,  0.0244],\n",
            "         [-0.1836,  0.0464, -0.0795,  0.0276,  0.2539],\n",
            "         [ 0.0649, -0.1654, -0.1222,  0.0394, -0.0118],\n",
            "         [ 0.0498,  0.0686, -0.1588, -0.4819,  0.0615],\n",
            "         [ 0.0842, -0.1527, -0.2091, -0.0427, -0.0245],\n",
            "         [ 0.0918, -0.0258, -0.1203, -0.1080, -0.0821],\n",
            "         [ 0.1548, -0.2320, -0.0302, -0.0133,  0.0572],\n",
            "         [ 0.0203, -0.0012, -0.1907, -0.4090,  0.0723],\n",
            "         [ 0.0214,  0.2132, -0.1522, -0.2455,  0.0977],\n",
            "         [-0.0165,  0.2876, -0.1233, -0.3390,  0.0212]]])\n",
            "output: tensor([[1, 2, 0, 1, 0, 3, 1, 4, 0, 1, 0, 0, 0, 4, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "colab = bool(os.getenv(\"COLAB_RELEASE_TAG\"))\n",
        "if not os.path.isfile('transformer.ipynb') and colab:\n",
        "    %pip install wget\n",
        "    %wget https://raw.githubusercontent.com/jaspock/minGPT/master/transformer.ipynb\n",
        "\n",
        "%pip install nbformat\n",
        "%run './transformer.ipynb'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 315\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "corpus = \"\"\"\n",
        "TEODORO:\t\n",
        "Fuese. ¿Quién pensó jamás\t\n",
        "de mujer tan noble y cuerda\t\n",
        "este arrojarse tan presto\t\n",
        "a dar su amor a entender?\t\n",
        "Pero también puede ser\t\n",
        "que yo me engañase en esto.\t\n",
        "Mas no me ha dicho jamás,\t\n",
        "ni a lo menos se me acuerda:\t\n",
        "«Pues ¿qué importa que se pierda,\t\n",
        "si se puede perder más?»\t\n",
        "Perder más... Bien puede ser\t\n",
        "por la mujer que decía...\t\n",
        "Mas todo es bachillería,\t\n",
        "y ella es la misma mujer.\t\n",
        "Aunque no, que la Condesa\t\n",
        "es tan discreta y tan varia\t\n",
        "que es la cosa más contraria\t\n",
        "de la ambición que profesa.\t\n",
        "Sírvenla príncipes hoy\t\n",
        "en Nápoles. ¿Qué no puedo\t\n",
        "ser su esclavo? Tengo miedo,\t\n",
        "que en grande peligro estoy.\t\n",
        "Ella sabe que a Marcela\t\n",
        "sirvo, pues aquí ha fundado\t\n",
        "el engaño y me ha burlado.\t\n",
        "Pero en vano se recela\t\n",
        "mi temor, porque jamás\t\n",
        "burlando salen colores.\t\n",
        "¿Y el decir con mil temores\t\n",
        "que se puede perder más?\t\n",
        "¿Qué rosa al llorar la Aurora\t\n",
        "hizo de las hojas ojos,\t\n",
        "abriendo los labios rojos\t\n",
        "con risa a ver cómo llora\t\n",
        "como ella los puso en mí,\t\n",
        "bañada en púrpura y grana,\t\n",
        "o qué pálida manzana\t\n",
        "se esmaltó de carmesí?\t\n",
        "Lo que veo y lo que escucho\t\n",
        "yo lo juzgo, o estoy loco,\t\n",
        "para ser de veras, poco,\t\n",
        "y para de burlas, mucho.\t\n",
        "Mas teneos, pensamiento,\t\n",
        "que os vais ya tras la grandeza,\t\n",
        "aunque si digo belleza\t\n",
        "bien sabéis vos que no miento,\t\n",
        "que es bellísima Diana\t\n",
        "y es discreción sin igual.\n",
        "\n",
        "MARCELA:\t\n",
        "¿Puedo hablarte?\n",
        "\n",
        "TEODORO:\t\n",
        "Ocasión tal\t\n",
        "mil imposibles allana,\t\n",
        "que por ti, Marcela mía,\t\n",
        "la muerte me es agradable.\n",
        "\n",
        "MARCELA:\t\n",
        "Como yo te vea y hable,\t\n",
        "dos mil vidas perdería.\t\n",
        "Estuve esperando el día\t\n",
        "como el pajarillo solo\t\n",
        "y, cuando vi que en el polo\t\n",
        "que Apolo más presto dora\t\n",
        "le despertaba la Aurora,\t\n",
        "dije: «Yo veré mi Apolo.»\t\n",
        "Grandes cosas han pasado,\t\n",
        "que no se quiso acostar\t\n",
        "la Condesa hasta dejar\t\n",
        "satisfecho su cuidado;\t\n",
        "amigas que han envidiado\t\n",
        "mi dicha con deslealtad\t\n",
        "le han contado la verdad,\t\n",
        "que entre quien sirve, aunque veas\t\n",
        "que hay amistad, no la creas,\t\n",
        "porque es fingida amistad.\t\n",
        "Todo lo sabe en efeto,\t\n",
        "que si es Diana la luna,\t\n",
        "siempre a quien ama importuna,\t\n",
        "salió y vio nuestro secreto;\t\n",
        "pero será, te prometo,\t\n",
        "para mayor bien, Teodoro,\t\n",
        "que del honesto decoro\t\n",
        "con que tratas de casarte\t\n",
        "le di parte, y dije aparte\t\n",
        "cuán tiernamente te adoro;\t\n",
        "tus prendas le encarecí,\t\n",
        "tu estilo, tu gentileza,\t\n",
        "y ella entonces su grandeza\t\n",
        "mostró tan piadosa en mí,\t\n",
        "que se alegró de que en ti\t\n",
        "hubiese los ojos puesto\t\n",
        "y de casarnos muy presto\t\n",
        "palabra también me dio,\t\n",
        "luego que de mí entendió\t\n",
        "que era tu amor tan honesto.\t\n",
        "Yo pensé que se enojara\t\n",
        "y la casa revolviera,\t\n",
        "que a los dos nos despidiera\t\n",
        "y a los demás castigara,\t\n",
        "mas su sangre ilustre y clara\t\n",
        "y aquel ingenio en efeto\t\n",
        "tan prudente y tan perfeto\t\n",
        "conoció lo que mereces.\t\n",
        "¡Oh, bien haya, amén mil veces,\t\n",
        "quien sirve a señor discreto!\n",
        "\n",
        "TEODORO:\t\n",
        "¿Que casarme prometió\t\n",
        "contigo?\n",
        "\n",
        "MARCELA:\t\n",
        "¿Pones duda\t\n",
        "que a su ilustre sangre acuda?\n",
        "\n",
        "TEODORO:\t\n",
        "Mi ignorancia me engañó\n",
        "\"\"\"\n",
        "\n",
        "word_list = list(set(corpus.split()))\n",
        "word_index = {'[PAD]': 0, '[UNK]': 1, '[EOS]': 2}\n",
        "special_tokens= len(word_index) \n",
        "for i, w in enumerate(word_list):\n",
        "    word_index[w] = i + special_tokens\n",
        "index_word = {i: w for i, w in enumerate(word_index)}\n",
        "vocab_size = len(word_index)\n",
        "print(\"vocab size: %d\" % vocab_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 0.12M\n",
            "Step     1, loss 5.781619\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step   100, loss 5.291278\n",
            "Step   200, loss 3.588163\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, outputs \u001b[39min\u001b[39;00m make_language_modeling_batch(corpus, word_index, max_len, batch_size, device):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/japerez/Dropbox/UA/activo/ciencia/me.github/me/docs/materials/transformers/assets/notebooks/lmgpt.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m eval_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/1567542203.py:19\u001b[0m, in \u001b[0;36mDecoderTransformer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# masked_fill will set to value (-inf) positions with True in the mask\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# triu will keep only the upper triangular part of the matrix, excluding the main diagonal because diagonal=1, and set to 0 the rest, \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# that is, the lower triangular part and the main diagonal\u001b[39;00m\n\u001b[1;32m     18\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtriu(torch\u001b[39m.\u001b[39mones(T, T, device\u001b[39m=\u001b[39mdevice), diagonal\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mbool()  \u001b[39m# causal attention mask   \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(inputs, mask)\n\u001b[1;32m     20\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x)\n\u001b[1;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/326512754.py:42\u001b[0m, in \u001b[0;36mAbstractTransformer.forward\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mdrop(tok_emb \u001b[39m+\u001b[39m pos_emb)\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh:\n\u001b[0;32m---> 42\u001b[0m     x \u001b[39m=\u001b[39m block(x, mask)\n\u001b[1;32m     43\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mln_f(x)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/3329532822.py:17\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x),mask)\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mc_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mc_fc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))))\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/1002560611.py:10\u001b[0m, in \u001b[0;36mMultiHeadAttentionSimple.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, padding_mask):\n\u001b[0;32m---> 10\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x, padding_mask) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# [(B,T,C')] -> (B, T, C)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(y))\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/1002560611.py:10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, padding_mask):\n\u001b[0;32m---> 10\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x, padding_mask) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# [(B,T,C')] -> (B, T, C)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(y))\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/tmp/ipykernel_6951/2650480206.py:11\u001b[0m, in \u001b[0;36mHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask): \n\u001b[1;32m     10\u001b[0m     B, T, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()  \u001b[39m# batch size, sequence length, main embedding dim, C' = head embedding dim\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_lin(x)  \u001b[39m# (B, T, C) -> (B, T, C')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(x)\n\u001b[1;32m     13\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/tpln/lib/python3.10/site-packages/torch/nn/modules/module.py:1682\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m   1675\u001b[0m \u001b[39m# On the return type:\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[39m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[39m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[39m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \u001b[39m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1683\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1684\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "n_head = 2\n",
        "n_embd =  64\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "batch_size = 4\n",
        "max_len = 32\n",
        "train_steps = 1000\n",
        "eval_steps = 100\n",
        "lr = 0.001\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = DecoderTransformer(n_embd=n_embd, n_head=n_head, n_layer=n_layer, vocab_size=vocab_size,  \n",
        "                max_len=max_len, embd_pdrop=embd_pdrop, attn_pdrop=attn_pdrop, resid_pdrop=resid_pdrop)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # not needed here since we are not padding inputs\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=train_steps, epochs=1, anneal_strategy='cos')\n",
        "\n",
        "model.train()\n",
        "step = 0\n",
        "\n",
        "for inputs, outputs in make_language_modeling_batch(corpus, word_index, max_len, batch_size, device):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(inputs)\n",
        "    loss = criterion(logits.view(-1,logits.size(-1)), outputs.view(-1)) \n",
        "    if (step + 1) % eval_steps == 0 or step == 0:\n",
        "        print(f\"Step {(step + 1):5d}, loss {loss.item():.6f}\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    step = step + 1\n",
        "    if (step==train_steps):\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEODORO: Cosas como estas son la cartilla, señora, de quien ama y quien desea. MARCELA: ¿Pones duda que a su ilustre sangre acuda? TEODORO: Mi ignorancia me engañó TEODORO: Fuese. ¿Quién pensó\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate_text(model, prompt, word_index, index_word, max_len, device):\n",
        "    words = prompt.split()\n",
        "    input_ids = [word_index.get(word, word_index['[UNK]']) for word in words]\n",
        "    input = torch.LongTensor(input_ids).view(1, -1).to(device)  # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(input_ids)):\n",
        "            output = model(input)\n",
        "            last_token_logits = output[0, -1, :]\n",
        "            predicted_id = torch.argmax(last_token_logits, dim=-1).item()\n",
        "            input = torch.cat([input, torch.LongTensor([predicted_id]).view(1,-1).to(device)], dim=1)\n",
        "            predicted_word = index_word[predicted_id]\n",
        "            words.append(predicted_word)\n",
        "            if predicted_word == '[EOS]':\n",
        "                break\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "model.eval()\n",
        "prompt = \"\"\"TEODORO: \n",
        "Cosas como estas\n",
        "son la cartilla, señora,\t\n",
        "de quien ama y quien desea.\n",
        "\n",
        "MARCELA:\"\"\"\n",
        "generated_text = generate_text(model, prompt, word_index, index_word, max_len, device)\n",
        "print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
