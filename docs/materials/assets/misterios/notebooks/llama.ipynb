{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yK6TSC8OfG"
      },
      "source": [
        "# Demo de dos modelos de lengua\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/assets/misterios/notebooks/llama.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> <a href=\"http://dlsi.ua.es/~japerez/\"><img src=\"https://img.shields.io/badge/Universitat-d'Alacant-5b7c99\" style=\"margin-left:10px\"></a>\n",
        "\n",
        "Cuaderno escrito por Juan Antonio P칠rez en 2025. Este cuaderno permite evaluar las continuaciones y las probabilidades de la siguiente palabra seg칰n dos modelos de lengua, uno entrenado 칰nicamente para predecir el siguiente token y otro entrenado para seguir instrucciones.\n",
        "\n",
        "El entorno de ejecuci칩n de este cuaderno ha de tener una GPU. En Google Colab, la puedes conseguir desde el men칰 *Entorno de ejecuci칩n* / *Cambiar tipo de entorno de ejecuci칩n*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMpSvW7yBlBm"
      },
      "source": [
        "## Inicializaci칩n de los modelos\n",
        "\n",
        "El siguiente c칩digo inicializa el modelo base y el modelo que sigue instrucciones a partir de dos modelos abiertos de tama침o 1.5B (1,5 millardos de par치metros). La descarga de los modelos puede llevar unos minutos, especialmente la primera vez que se ejecute el cuaderno en un nuevo entorno de ejecuci칩n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egT2Bnqt2182"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Base model setup\n",
        "# base_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B\"\n",
        "base_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=base_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
        "                                                  torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "# Instruction-tuned model setup\n",
        "# instruction_model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "instruction_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "instruction_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=instruction_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "instruction_tokenizer = AutoTokenizer.from_pretrained(instruction_model_id)\n",
        "instruction_model = AutoModelForCausalLM.from_pretrained(instruction_model_id,\n",
        "                                                         torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jQDVzlm0VPJ"
      },
      "source": [
        "## Obtenci칩n de las probabilidades de salida\n",
        "\n",
        "La funci칩n `get_word_probs` devuelve un diccionario de Python con las probabilidades del modelo para las palabras de la lista `candidates` cuando se ha procesado el contexto `text` con el modelo `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO_eZRt-ozG-"
      },
      "outputs": [],
      "source": [
        "def get_word_probs(model, tokenizer, text, candidates):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[0, -1]  # last token logits\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    candidate_probs = {}  # initial dictionary\n",
        "    for word in candidates:\n",
        "        token_id = tokenizer.convert_tokens_to_ids(word)\n",
        "        if token_id is not None:\n",
        "            candidate_probs[word] = probs[token_id].item()\n",
        "        else:\n",
        "            candidate_probs[word] = 0.0  # word not in embedding table\n",
        "\n",
        "    return candidate_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaEcUBhQ9_qs"
      },
      "source": [
        "## Probando, probando...\n",
        "\n",
        "En primer lugar, probaremos que los modelos funcionan pidi칠ndoles que contin칰en un texto.\n",
        "\n",
        "Si el valor de `do_sample` est치 a `True`, el sistema genera salidas diferentes cada vez. La diversidad en ese caso se puede ajustar jugando con los argumentos siguientes:\n",
        "\n",
        "- `top_k`: limita el n칰mero de palabras (tokens) m치s probables a considerar; por ejemplo, `top_k=50`\n",
        "- `top_p`: elige palabras hasta alcanzar una probabilidad acumulada espec칤fica; por ejemplo, `top_p=0.95`\n",
        "- `temperature`: controla la aleatoriedad: valores bajos entre 0 y 1 son m치s conservadores, altos entre 1 y 2 m치s creativos, pero aumentando el riesgo de incoherencias; es habitual hacer `temperature=0.7`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-JbDD36-AEW"
      },
      "outputs": [],
      "source": [
        "question = \"What is the capital of France?\"\n",
        "\n",
        "inputs_normal = base_tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "response_normal = base_model.generate(**inputs_normal, max_new_tokens=50)\n",
        "print(\"游몱 Response of the base model:\" , base_tokenizer.decode(response_normal[0], skip_special_tokens=True, do_sample=True))\n",
        "\n",
        "inputs_instr = instruction_tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n",
        "response_instr = instruction_model.generate(**inputs_instr, max_new_tokens=50)\n",
        "print(\"游몱 Response of the instruction-tuned model: \", instruction_tokenizer.decode(response_instr[0], skip_special_tokens=True, do_sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9EJtiDI2Ymj"
      },
      "source": [
        "## Obtenci칩n de probabilidades\n",
        "\n",
        "A continuaci칩n, usaremos los modelos para obtener las probabilidades de la siguiente palabra una vez procesado el texto de `example-text`, pero solo para aquellas palabras de la lista `candidate_words`.\n",
        "\n",
        "En realidad, los modelos de lengua no trabajan necesariamente a nivel de palabra, sino que por ciertos motivos que no vienen al caso, trocean las palabras pocos frecuentes en fragmentos que s칤 son frecuentes. A los elementos del vocabulario resultantes (que pueden ser palabras completas o fragmentos de ellas) se les denomina *tokens*.\n",
        "\n",
        "Por ello, aseg칰rate de que `candidate_words` contiene palabras que s칤 est치n en el vocabulario; una manera r치pida de comprobarlo es verificar que no se devuelve 0.0 para su probabilidad, ya que es lo que hace el programa si la palabra no est치 en el vocabulario. Aunque en un principio podr칤a parecer que esto no nos permite diferenciar entre palabras que no est치n en el vocabulario y palabras que s칤 lo est치n pero tienen probabilidad nula, lo cierto es que, por las funciones matem치ticas que usa la red neuronal, la salida de las palabras poco probables ser치 un valor muy peque침o, pero nunca cero.\n",
        "\n",
        "Observa que las probabilidades se presentan en notaci칩n cient칤fica, por ejemplo, `7.59027898311615e-08`. Esto equivale a $7.59027898311615 \\times 10^{-8}$ o, lo que es lo mismo, 0.0000000759027898311615.\n",
        "\n",
        "Aunque el c칩digo obtiene el valor de las probabilidades de que las palabras de `candidate_words` sean la palabra que sigue a `example_text`, lo cierto es que este enfoque no es completamente v치lido para el modelo que sigue instrucciones, ya que este espera que la entrada siga una determinada plantilla con las instrucciones generales dadas al modelo (`system`) y la entrada del usuario (`user`). Por ello, adem치s de simplemente obtener las probabilidades emitidas por la red neuronal (algo que puede dar resultados m치s o menos correctos, ya que el modelo fue entrenado en su fase inicial para predecir el siguiente token), usamos esta plantilla para pedir al modelo una continuaci칩n siguiendo las mismas pautas con las que ha sido entrenado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33c-CHdSpz4y"
      },
      "outputs": [],
      "source": [
        "example_text = \"The weather in this summer day is \"\n",
        "candidate_words = [\"hot\", \"cold\", \"nice\", \"car\"]\n",
        "\n",
        "probs_base = get_word_probs(base_model, base_tokenizer, example_text, candidate_words)\n",
        "print(\"游몱 Probabilities of the base model:\", probs_base)\n",
        "\n",
        "instruction_text = example_text\n",
        "probs_instr = get_word_probs(instruction_model, instruction_tokenizer, instruction_text, candidate_words)\n",
        "print(\"游몱 Probabilities of the instruction-tuned model with no template:\", probs_instr)\n",
        "\n",
        "system = \"You are an expert who knows how to complete every incomplete sentence.\"\n",
        "prompt1 = \"Hi! I will ask you later about the following sentence: '\" + example_text + \"'. For now, just say that you agree.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": prompt1},\n",
        "]\n",
        "text = instruction_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "inputs = instruction_tokenizer([text], return_tensors=\"pt\").to(instruction_model.device)\n",
        "\n",
        "generated_ids = instruction_model.generate(**inputs, max_new_tokens=40)\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response1 = instruction_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"游몱 First output of the instruction-tuned model following the template:\", response1)\n",
        "\n",
        "prompt2= \"Gime me four comma-separated words that would likely continue the sentence I gave you before.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system},\n",
        "    {\"role\": \"user\", \"content\": prompt1},\n",
        "    {\"role\": \"assistant\", \"content\": response1},\n",
        "    {\"role\": \"user\", \"content\": prompt2},\n",
        "]\n",
        "text = instruction_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "inputs = instruction_tokenizer([text], return_tensors=\"pt\").to(instruction_model.device)\n",
        "\n",
        "generated_ids = instruction_model.generate(**inputs, max_new_tokens=40)\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response2 = instruction_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"游몱 Second output of the instruction-tuned model following the template:\", response2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3S1nd2m_juO"
      },
      "source": [
        "## Generaci칩n de respuestas\n",
        "\n",
        "Por 칰ltimo, generaremos m치s salidas. En el caso del modelo que sigue instrucciones, usaremos la plantilla que se us칩 durante su entrenamiento para obtener mejores resultados.\n",
        "\n",
        "Para las personas muy observadoras, en el c칩digo anterior hemos usado por separado el *tokenizador* (que obtiene los *tokens* asociados a las palabras) y el modelo en s칤. En el c칩digo siguiente, usaremos un *pipeline* que simplifica el c칩digo para obtener la salida del modelo, pero no nos permite obtener de forma directa las probabilidades de salida que nos interesaban antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPvSj3rj5AF5"
      },
      "outputs": [],
      "source": [
        "output = base_pipe(\"The key to life is\", max_new_tokens=40, do_sample=True)\n",
        "print(\"游몱 Response of the base model: \", output[0][\"generated_text\"])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "outputs = instruction_pipe(messages, max_new_tokens=40, do_sample=True)\n",
        "print(\"游몱 Response of the instruction-tuned model: \", outputs[0][\"generated_text\"][-1])   # print only new output\n",
        "\n",
        "# call apply_chat_template if not using the pipeline:\n",
        "# prompt = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
